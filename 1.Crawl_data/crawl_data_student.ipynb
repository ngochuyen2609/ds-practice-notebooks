{"cells":[{"cell_type":"markdown","id":"92692da7","metadata":{"id":"92692da7"},"source":["#  Thực Hành Crawl Dữ Liệu Tin Tức từ VNExpress  \n","#  Practice: Crawling News Data from VNExpress  \n","\n","---\n","\n","## Giới thiệu  \n","##  Introduction  \n","\n","Chào mừng các bạn đến với bài thực hành về **Web Crawling** (thu thập dữ liệu web)!  \n","Welcome to the practical exercise on **Web Crawling** (collecting data from websites)!  \n","\n","Trong notebook này, chúng ta sẽ học cách thu thập dữ liệu tin tức từ **VNExpress** — một trong những trang báo trực tuyến hàng đầu Việt Nam.  \n","In this notebook, we will learn how to collect news data from **VNExpress** — one of Vietnam’s leading online newspapers.  \n","\n","---\n","\n","## Mục tiêu học tập  \n","## Learning Objectives  \n","\n","Sau khi hoàn thành bài thực hành này, bạn sẽ có thể:  \n","After completing this exercise, you will be able to:  \n","\n","1. **Hiểu về Web Crawling** – Nắm vững khái niệm thu thập dữ liệu từ web và các ứng dụng thực tế.  \n","   **Understand Web Crawling** – Grasp the concept of collecting web data and its real-world applications.  \n","\n","2. **Sử dụng các thư viện Python** – Thành thạo BeautifulSoup, requests, và các công cụ crawling khác.  \n","   **Use Python Libraries** – Become proficient in BeautifulSoup, requests, and other crawling tools.  \n","\n","3. **Xử lý dữ liệu HTML** – Biết cách phân tích cấu trúc HTML và trích xuất thông tin.  \n","   **Process HTML Data** – Learn to analyze HTML structures and extract useful information.  \n","\n","4. **Quản lý dữ liệu** – Lưu trữ và tổ chức dữ liệu crawl được một cách hiệu quả.  \n","   **Manage Data** – Store and organize crawled data efficiently.  \n","\n","5. **Xử lý lỗi** – Nhận biết và khắc phục các vấn đề thường gặp khi crawl dữ liệu.  \n","   **Handle Errors** – Identify and resolve common issues encountered during crawling.  \n","\n","---\n","\n","##  Ứng dụng thực tế  \n","## Practical Applications  \n","\n","- **Phân tích xu hướng tin tức** – Thu thập dữ liệu để phân tích các chủ đề hot.  \n","  **News Trend Analysis** – Collect data to analyze trending topics.  \n","\n","- **Nghiên cứu học thuật** – Thu thập corpus văn bản tiếng Việt để nghiên cứu NLP.  \n","  **Academic Research** – Gather Vietnamese text corpora for NLP research.  \n","\n","- **Theo dõi thương hiệu** – Giám sát thông tin về công ty/sản phẩm trên báo chí.  \n","  **Brand Monitoring** – Track company or product mentions in the media.  \n","\n","- **Machine Learning** – Tạo dataset để train model phân loại văn bản.  \n","  **Machine Learning** – Create datasets to train text classification models.  \n","\n","---\n","\n","##  Lưu ý quan trọng  \n","##  Important Notes  \n","\n","- **Tuân thủ robots.txt** – Luôn kiểm tra quy định của website trước khi crawl.  \n","  **Respect robots.txt** – Always check the website’s crawling policy before collecting data.  \n","\n","- **Crawl có trách nhiệm** – Không gây quá tải cho server bằng cách thêm delay giữa các request.  \n","  **Crawl Responsibly** – Avoid overloading servers by adding delays between requests.  \n","\n","- **Chỉ sử dụng cho mục đích học tập** – Dữ liệu crawl được chỉ dùng để học và nghiên cứu.  \n","  **For Educational Use Only** – The crawled data should only be used for learning and research.  \n","\n","---\n","\n","## Yêu cầu  \n","## Requirements  \n","\n","Sinh viên cần hoàn thiện file code tại các phần `#TODO` và crawl được dữ liệu các bài báo từ trang web VNExpress.  \n","Students must complete the code sections marked with `#TODO` and crawl news articles from the VNExpress website.  \n","\n","- **Đầu ra** là các file dữ liệu thô HTML và các file dữ liệu được lưu dưới định dạng `*.txt`.  \n","- **Output** should include raw HTML data files and processed text files saved as `*.txt`.  \n"]},{"cell_type":"markdown","id":"f20105d2","metadata":{"id":"f20105d2"},"source":["# Chuẩn bị môi trường  \n","# Environment Setup  \n","\n","---\n","\n","## Cài đặt thư viện cần thiết  \n","## Install Required Libraries  \n","\n","Trước khi bắt đầu crawl dữ liệu, chúng ta cần cài đặt các thư viện Python cần thiết.  \n","Before starting the data crawling process, we need to install the required Python libraries.  \n","\n","Mỗi thư viện có vai trò riêng trong quá trình thu thập và xử lý dữ liệu:  \n","Each library serves a specific role in data collection and processing:  \n","\n","---\n","\n","### Danh sách thư viện và chức năng  \n","### List of Libraries and Their Functions  \n","\n","- **`beautifulsoup4`**: Thư viện chính để phân tích HTML và XML, giúp trích xuất dữ liệu từ các thẻ HTML.  \n","  **`beautifulsoup4`**: The main library for parsing HTML and XML, used to extract data from HTML tags.  \n","\n","- **`requests`**: Gửi HTTP requests để lấy nội dung trang web.  \n","  **`requests`**: Sends HTTP requests to retrieve webpage content.  \n","\n","- **`PyYAML`**: Xử lý file cấu hình định dạng YAML.  \n","  **`PyYAML`**: Handles configuration files in YAML format.  \n","\n","- **`tqdm`**: Hiển thị thanh tiến trình khi crawl nhiều trang.  \n","  **`tqdm`**: Displays progress bars when crawling multiple pages.  \n","\n","- **`pandas`**: Xử lý và phân tích dữ liệu dạng bảng.  \n","  **`pandas`**: Used for handling and analyzing tabular data.  \n","\n","- **`numpy`**: Hỗ trợ tính toán với mảng và ma trận.  \n","  **`numpy`**: Supports numerical computation with arrays and matrices.  \n","\n","---\n","\n",">  **Gợi ý (Tip)**:  \n","> Trên Google Colab, hầu hết các thư viện này đã được cài sẵn, nhưng chúng ta vẫn chạy lệnh cài đặt để đảm bảo có phiên bản mới nhất.  \n","> On Google Colab, most of these libraries are already pre-installed, but we’ll run the installation command to ensure we have the latest versions.  \n"]},{"cell_type":"code","execution_count":null,"id":"79cd36a9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79cd36a9","outputId":"21721c09-8d12-4ac1-a54b-36469eafd914","executionInfo":{"status":"ok","timestamp":1759682730729,"user_tz":-420,"elapsed":29806,"user":{"displayName":"Hai Pham Class","userId":"07688336479798365055"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"]}],"source":["# Install the necessary libraries (Cài đặt các thư viện cần thiết)\n","!pip install beautifulsoup4\n","!pip install PyYAML\n","!pip install requests\n","!pip install tqdm\n","!pip install pandas\n","!pip install numpy"]},{"cell_type":"markdown","id":"41ffe9ba","metadata":{"id":"41ffe9ba"},"source":["# Nhập thư viện\n","# Import Libraries\n","\n","Sau khi cài đặt xong, chúng ta cần import các thư viện cần thiết để sử dụng trong mã. Hãy cùng tìm hiểu vai trò của từng thư viện.  \n","After installation, we need to import the necessary libraries to use in our code. Let’s explore the role of each one.\n","\n","---\n","\n","### Nhóm thư viện cơ bản\n","### Basic Libraries\n","- **`os`**: Thao tác với hệ thống file (tạo thư mục, xử lý đường dẫn)  \n","  **`os`**: Work with the file system (create folders, handle file paths)\n","- **`json`**: Xử lý dữ liệu định dạng JSON  \n","  **`json`**: Handle JSON data\n","- **`time`**: Tạo độ trễ giữa các yêu cầu để tránh bị chặn  \n","  **`time`**: Create delays between requests to avoid being blocked\n","- **`re`**: Dùng biểu thức chính quy để xử lý chuỗi  \n","  **`re`**: Use regular expressions to process strings\n","\n","---\n","\n","### Nhóm thư viện thu thập dữ liệu web\n","### Web Crawling Libraries\n","- **`requests`**: Gửi yêu cầu HTTP để lấy nội dung trang web  \n","  **`requests`**: Send HTTP requests to fetch web content\n","- **`BeautifulSoup`**: Phân tích HTML và trích xuất dữ liệu  \n","  **`BeautifulSoup`**: Parse HTML and extract information\n","- **`tqdm`**: Hiển thị thanh tiến trình  \n","  **`tqdm`**: Display progress bars\n","\n","---\n","\n","### Nhóm thư viện xử lý dữ liệu\n","### Data Processing Libraries\n","- **`yaml`**: Đọc các tệp cấu hình  \n","  **`yaml`**: Read configuration files\n","- **`typing`**: Cung cấp type hint giúp code rõ ràng hơn  \n","  **`typing`**: Provide type hints for clearer code\n","- **`urllib.parse`**: Xử lý URL  \n","  **`urllib.parse`**: Handle URLs\n","\n","---\n","\n","> **Lưu ý**: Hãy đảm bảo chạy cell cài đặt thư viện trước khi chạy cell import này!  \n","> **Note**: Make sure to run the library installation cell before running this import cell!\n"]},{"cell_type":"code","execution_count":null,"id":"d1fa12f8","metadata":{"id":"d1fa12f8"},"outputs":[],"source":["import os\n","import json\n","import yaml\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm import tqdm\n","import time\n","from typing import Dict, List\n","import re\n","from urllib.parse import urljoin, urlparse\n","import random"]},{"cell_type":"markdown","id":"eb6e0040","metadata":{"id":"eb6e0040"},"source":["# Kết nối và lấy dữ liệu từ website  \n","# Connecting and Fetching Data from the Website\n","\n","## Định nghĩa các hàm crawl dữ liệu  \n","## Defining Crawling Functions\n","\n","Trong phần này, chúng ta sẽ tạo ra các hàm để thu thập dữ liệu từ VNExpress. Quá trình crawl dữ liệu bao gồm các bước:  \n","In this section, we will create functions to collect data from VNExpress. The crawling process includes the following steps:\n","\n","1. **Gửi request** đến trang web  \n","   **Send a request** to the website\n","2. **Nhận response** HTML  \n","   **Receive the HTML response**\n","3. **Phân tích HTML** bằng BeautifulSoup  \n","   **Parse the HTML** using BeautifulSoup\n","4. **Trích xuất thông tin** cần thiết  \n","   **Extract the required information**\n","\n","---\n","\n","### Hàm `get_content_()` - Lấy nội dung HTML cơ bản  \n","### Function `get_content_()` - Basic HTML fetching\n","\n","**Mục đích**: Tải nội dung HTML từ một URL với các kỹ thuật anti-detection  \n","**Purpose**: Download HTML content from a URL using anti-detection techniques\n","\n","**Đầu vào**:  \n","**Input**:\n","- `url` (string): URL của trang web cần crawl  \n","  `url` (string): The URL of the webpage to crawl\n","\n","**Đầu ra**:  \n","**Output**:\n","- HTML content (string) nếu thành công  \n","  HTML content (string) if successful\n","- `None` nếu thất bại  \n","  `None` if failed\n","\n","**Kỹ thuật chống phát hiện**:  \n","**Anti-detection techniques**:\n","- **User-Agent rotation**: Thay đổi User-Agent để giả lập các trình duyệt khác nhau  \n","  **User-Agent rotation**: Rotate User-Agent strings to mimic different browsers\n","- **Headers giả lập**: Sử dụng headers giống trình duyệt thực  \n","  **Browser-like headers**: Use headers that resemble a real browser\n","- **Random delay**: Tạo độ trễ ngẫu nhiên giữa các request  \n","  **Random delay**: Introduce random delays between requests\n","- **Retry mechanism**: Thử lại tối đa 3 lần nếu thất bại  \n","  **Retry mechanism**: Retry up to 3 times on failure\n","\n","> **Tại sao cần chống phát hiện?**  \n","> **Why is anti-detection needed?**\n","\n","> Nhiều website có hệ thống chống bot để bảo vệ server khỏi bị quá tải. Chúng ta cần crawl một cách \"lịch sự\" để không bị chặn.  \n","> Many websites employ anti-bot measures to protect servers from overload. We should crawl politely to avoid being blocked.\n"]},{"cell_type":"code","execution_count":null,"id":"1f4c88fc","metadata":{"id":"1f4c88fc"},"outputs":[],"source":["def get_content_(url):\n","    \"\"\"\n","    This function is responsible for:\n","    Fetching the HTML content from a given URL,\n","    with an anti-bot detection mechanism.\n","    (Hàm này có nhiệm vụ:\n","    Lấy nội dung HTML từ một URL,\n","    có cơ chế chống bị phát hiện là bot.)\n","    \"\"\"\n","\n","    # Import additional libraries locally to avoid global conflicts\n","    # (Import thêm các thư viện cần thiết, đặt trong hàm để tránh xung đột toàn cục)\n","    import random\n","    import ssl\n","    from urllib.parse import urlparse\n","\n","    # Clean URL: remove the fragment part after '#' if it exists\n","    # (Làm sạch URL: nếu có dấu # thì loại bỏ phần sau nó)\n","    if '#' in url:\n","        url = url.split('#')[0]\n","\n","    # Validate URL: it must start with \"http\"\n","    # (Kiểm tra URL hợp lệ: phải bắt đầu bằng “http”)\n","    if not url.startswith('http'):\n","        return None  # Return None if the URL is invalid (Trả về None nếu URL sai)\n","\n","    # A list of different User-Agents to randomly rotate (helps avoid blocking)\n","    # (Danh sách nhiều User-Agent khác nhau để chọn ngẫu nhiên, giúp tránh bị chặn)\n","    user_agents = [\n","        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n","        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n","        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n","        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n","    ]\n","\n","    headers = {\n","    # Declare accepted content types (HTML, XML, images, etc.)\n","    # Khai báo các loại nội dung mà client có thể nhận (HTML, XML, hình ảnh, v.v.)\n","    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n","\n","    # Preferred languages for response (Vietnamese first, then English, French)\n","    # Ngôn ngữ ưu tiên khi nhận phản hồi — ưu tiên tiếng Việt, sau đó tiếng Anh và tiếng Pháp\n","    'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7,fr;q=0.6',\n","\n","    # Allow data compression for faster transfers (gzip, deflate, br)\n","    # Cho phép nén dữ liệu để tải nhanh hơn (gzip, deflate, br)\n","    'Accept-Encoding': 'gzip, deflate, br',\n","\n","    # \"Do Not Track\" header requests not to be tracked by websites\n","    # Header “Do Not Track” yêu cầu website không theo dõi hoạt động của người dùng\n","    'DNT': '1',\n","\n","    # Keep the TCP connection alive for multiple requests\n","    # Giữ kết nối TCP mở để có thể gửi nhiều yêu cầu liên tiếp, tăng hiệu suất\n","    'Connection': 'keep-alive',\n","\n","    # Allow upgrade from HTTP to HTTPS if supported\n","    # Cho phép nâng cấp từ HTTP lên HTTPS nếu máy chủ hỗ trợ\n","    'Upgrade-Insecure-Requests': '1',\n","\n","    # Fetch metadata: destination is a document (webpage)\n","    # Thông tin fetch: đích đến là tài liệu (trang web)\n","    'Sec-Fetch-Dest': 'document',\n","\n","    # Fetch metadata: mode is “navigate” (simulates user navigation)\n","    # Thông tin fetch: chế độ là “navigate” (mô phỏng hành vi người dùng nhấp vào liên kết)\n","    'Sec-Fetch-Mode': 'navigate',\n","\n","    # Fetch metadata: request origin — “none” means direct access (not from iframe)\n","    # Thông tin fetch: nguồn request — “none” nghĩa là truy cập trực tiếp (không qua trang khác)\n","    'Sec-Fetch-Site': 'none',\n","\n","    # Indicates a real user interaction (?1 = true)\n","    # Biểu thị có tương tác người dùng thật (?1 = có)\n","    'Sec-Fetch-User': '?1',\n","\n","    # Force revalidation: no cache reuse (always get the latest content)\n","    # Buộc tải lại nội dung mới, không dùng dữ liệu lưu trong cache\n","    'Cache-Control': 'max-age=0',\n","\n","    # Client hints: describe browser family and version\n","    # Gợi ý trình duyệt: mô tả họ và phiên bản của trình duyệt (ví dụ Chrome)\n","    'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n","\n","    # Client hints: specify if device is mobile (?0 = no)\n","    # Gợi ý trình duyệt: xác định có phải thiết bị di động không (?0 = không)\n","    'sec-ch-ua-mobile': '?0',\n","\n","    # Client hints: specify operating system / platform (e.g., macOS)\n","    # Gợi ý trình duyệt: cho biết hệ điều hành hoặc nền tảng (ví dụ: macOS)\n","    'sec-ch-ua-platform': '\"macOS\"',\n","\n","    # Randomly select a User-Agent to simulate different browsers\n","    # Chọn ngẫu nhiên một User-Agent để giả lập nhiều loại trình duyệt, tránh bị chặn\n","    'User-Agent': random.choice(user_agents),\n","\n","    # Indicate the referring page (where the request comes from)\n","    # Chỉ ra trang mà người dùng đến từ đó (nguồn truy cập)\n","    'Referer': 'https://vnexpress.net/',\n","\n","    # Specify the origin domain of the request (for CORS / CSRF validation)\n","    # Xác định nguồn gốc domain của request (dùng trong xác thực CORS/CSRF)\n","    'Origin': 'https://vnexpress.net'\n","    }\n","\n","\n","    # Create a session to maintain cookies and headers across multiple requests\n","    # (Tạo một session để giữ cookie và headers giữa các yêu cầu HTTP)\n","    session = requests.Session()\n","\n","    # Disable SSL verification (some sites may have invalid certificates)\n","    # (Tắt xác thực SSL vì một số trang có thể bị lỗi chứng chỉ)\n","    session.verify = False\n","\n","    # Suppress SSL warning messages from showing in the console\n","    # (Ẩn cảnh báo SSL để không in ra màn hình)\n","    import urllib3\n","    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n","\n","    # Update session headers with our custom headers\n","    # (Cập nhật headers giả lập vào session)\n","    session.headers.update(headers)\n","\n","    # Try sending the request up to 3 times in case of failure\n","    # (Thử gửi yêu cầu tối đa 3 lần, nếu lỗi thì thử lại)\n","    for attempt in range(3):\n","        try:\n","            # Add random delay between requests to avoid being rate-limited\n","            # (Thêm độ trễ ngẫu nhiên giữa các lần request để tránh bị chặn)\n","            time.sleep(random.uniform(1, 3))\n","\n","            # Send the GET request\n","            # (Gửi yêu cầu GET tới trang web)\n","            response = session.get(\n","                url,\n","                timeout=30,           # Max wait time = 30s (Giới hạn tối đa 30 giây)\n","                allow_redirects=True, # Follow redirects automatically (Cho phép tự động theo redirect)\n","                stream=False          # Download all content at once (Không tải kiểu stream)\n","            )\n","\n","            # If successful response (status code 200)\n","            # (Nếu phản hồi thành công — mã 200)\n","            if response.status_code == 200:\n","                response.encoding = 'utf-8'  # Set UTF-8 encoding for Vietnamese text (Đặt mã hóa UTF-8)\n","                content = response.text      # Extract HTML content (Lấy nội dung HTML)\n","                response.close()\n","                session.close()\n","                return content               # Return HTML content (Trả kết quả về)\n","\n","            # If access is forbidden (HTTP 403)\n","            # (Nếu bị từ chối truy cập — lỗi 403)\n","            elif response.status_code == 403:\n","                # Change User-Agent and retry\n","                # (Đổi User-Agent và thử lại)\n","                headers['User-Agent'] = random.choice(user_agents)\n","                session.headers.update(headers)\n","                time.sleep(random.uniform(2, 5))  # Wait 2–5 seconds before retrying (Nghỉ 2–5 giây rồi thử lại)\n","                continue\n","\n","            # If other HTTP errors occur\n","            # (Nếu gặp mã lỗi HTTP khác)\n","            else:\n","                if attempt == 2:\n","                    print(f\"HTTP {response.status_code} for {url}\")\n","                response.close()\n","\n","        # Handle SSL errors\n","        # (Xử lý lỗi SSL)\n","        except requests.exceptions.SSLError:\n","            continue  # Retry (Thử lại)\n","\n","        # Handle general request errors (timeouts, connection issues, etc.)\n","        # (Xử lý lỗi request chung: hết thời gian chờ, lỗi kết nối,...)\n","        except requests.exceptions.RequestException as e:\n","            if attempt == 2:\n","                print(f\"Request error for {url}: {e}\")\n","\n","        # Handle any unexpected errors\n","        # (Xử lý lỗi không xác định)\n","        except Exception as e:\n","            if attempt == 2:\n","                print(f\"Unknown error for {url}: {e}\")\n","\n","        # Increase delay between retries to avoid server rate limits\n","        # (Tăng thời gian chờ giữa các lần thử lại để tránh bị giới hạn tốc độ)\n","        if attempt < 2:\n","            time.sleep(random.uniform(3 + attempt, 6 + attempt))\n","\n","    # After 3 failed attempts, close the session and return None\n","    # (Sau 3 lần thử thất bại, đóng session và trả về None)\n","    session.close()\n","    return None\n"]},{"cell_type":"markdown","id":"1b1ba03e","metadata":{"id":"1b1ba03e"},"source":["### Hàm backup với cURL  \n","### Backup Function Using cURL  \n","\n","**Tại sao cần phương pháp backup?**  \n","**Why do we need a backup method?**  \n","\n","Đôi khi website có thể chặn Python requests nhưng vẫn cho phép truy cập từ các tool khác như cURL. Đây là lý do chúng ta cần có phương pháp dự phòng.  \n","Sometimes, websites may block Python requests but still allow access from other tools such as cURL. That’s why we need a fallback method.\n","\n","**Đặc điểm của các hàm này**:  \n","**Characteristics of these functions**:  \n","- `get_content_with_curl()`: Sử dụng lệnh cURL của hệ thống  \n","  `get_content_with_curl()`: Uses the system’s cURL command  \n","- `get_content_enhanced()`: Kết hợp cả hai phương pháp (requests + cURL)  \n","  `get_content_enhanced()`: Combines both methods (requests + cURL)\n","\n","> **Thực hành**: Thử chạy từng hàm riêng lẻ với cùng một URL để thấy sự khác biệt!  \n","> **Practice**: Try running each function separately with the same URL to observe the difference!\n"]},{"cell_type":"code","execution_count":null,"id":"e813d6e0","metadata":{"id":"e813d6e0","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1759682730775,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hai Pham Class","userId":"07688336479798365055"}},"outputId":"53c62bd3-0aab-470a-9cbc-7839f59f003e"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-268203055.py, line 90)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-268203055.py\"\u001b[0;36m, line \u001b[0;32m90\u001b[0m\n\u001b[0;31m    content = #TODO\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["def get_content_with_curl(url):\n","    \"\"\"\n","    Backup method using cURL to bypass anti-bot systems.\n","    -> If 'requests' fails or gets blocked, we use the external curl command instead.\n","    (Phương thức dự phòng sử dụng cURL để vượt qua hệ thống chống bot.\n","    → Nếu requests bị chặn, ta sẽ gọi lệnh curl bên ngoài hệ thống.)\n","    \"\"\"\n","    import subprocess   # Used to run system commands (like 'curl') (Dùng để gọi lệnh hệ thống như 'curl')\n","    import tempfile     # Used to create temporary files (Dùng để tạo file tạm lưu dữ liệu tải về)\n","    import os           # Used for file operations (Dùng để xóa file tạm sau khi sử dụng)\n","\n","    # Clean URL: remove any fragment part after '#'\n","    # (Làm sạch URL: loại bỏ phần sau dấu '#')\n","    if '#' in url:\n","        url = url.split('#')[0]\n","\n","    # Validate URL: must start with 'http' or 'https'\n","    # (Kiểm tra tính hợp lệ: URL phải bắt đầu bằng http hoặc https)\n","    if not url.startswith('http'):\n","        return None\n","\n","    try:\n","        # Create a temporary file to store the downloaded HTML\n","        # (Tạo file tạm thời để lưu kết quả tải về)\n","        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.html') as temp_file:\n","            temp_filename = temp_file.name  # Save file path for later use (Lưu tên file để sử dụng sau)\n","\n","        # Build the curl command to simulate a real browser (like Chrome)\n","        # (Tạo lệnh curl giả lập trình duyệt thật — ví dụ như Chrome)\n","        curl_command = [\n","            'curl',\n","            '-s',  # Silent mode: no progress bar or logs (Chế độ im lặng — không in log ra terminal)\n","            '-L',  # Follow redirects automatically (Tự động theo dõi các redirect)\n","            '--compressed',  # Enable compressed transfer (Cho phép tải nội dung nén)\n","            '--max-time', '30',  # Timeout after 30 seconds (Giới hạn thời gian 30 giây)\n","            '--retry', '2',      # Retry twice if request fails (Thử lại tối đa 2 lần nếu lỗi)\n","            '--user-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n","            # Use a realistic User-Agent to avoid detection (Giả lập trình duyệt thật để tránh bị chặn)\n","\n","            # Common browser headers (Các header phổ biến mà trình duyệt gửi kèm)\n","            '--header', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n","            '--header', 'Accept-Language: vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',\n","            '--header', 'Accept-Encoding: gzip, deflate, br',\n","            '--header', 'DNT: 1',  # Do Not Track header (Yêu cầu không theo dõi)\n","            '--header', 'Connection: keep-alive',\n","            '--header', 'Upgrade-Insecure-Requests: 1',\n","            '--header', 'Sec-Fetch-Dest: document',\n","            '--header', 'Sec-Fetch-Mode: navigate',\n","            '--header', 'Sec-Fetch-Site: none',\n","            '--header', 'Sec-Fetch-User: ?1',\n","            '--header', 'Cache-Control: max-age=0',\n","            '--referer', 'https://vnexpress.net/',  # Pretend to come from vnexpress homepage (Giả vờ người dùng click từ trang chủ)\n","            '--output', temp_filename,              # Save output HTML to temp file (Ghi nội dung tải về vào file tạm)\n","            url\n","        ]\n","\n","        # Execute the curl command as a subprocess\n","        # (Chạy lệnh curl bên ngoài Python)\n","        result = subprocess.run(\n","            curl_command,            # Command to run (Lệnh cần chạy)\n","            capture_output=True,     # Capture stdout/stderr (Lưu stdout và stderr thay vì in ra)\n","            text=True,               # Return output as text (Trả kết quả về dạng chuỗi)\n","            timeout=35               # Total timeout for curl (Giới hạn thời gian tổng 35 giây)\n","        )\n","\n","        # If curl executed successfully (exit code = 0)\n","        # (Nếu curl chạy thành công — mã thoát = 0)\n","        if result.returncode == 0:\n","            # Read the downloaded HTML content from temp file\n","            # (Đọc nội dung HTML từ file tạm)\n","            with open(temp_filename, 'r', encoding='utf-8', errors='ignore') as f:\n","                content = f.read()\n","\n","            # Remove the temp file after reading\n","            # (Xóa file tạm sau khi sử dụng)\n","            os.unlink(temp_filename)\n","\n","            # Check if the content is long enough to be valid\n","            # (Kiểm tra xem nội dung có đủ dài để coi là hợp lệ không)\n","            if len(content) > 1000:\n","                return content\n","\n","        # Cleanup: if temp file still exists, delete it\n","        # (Dọn dẹp: nếu file tạm vẫn còn, xóa nó đi)\n","        if os.path.exists(temp_filename):\n","            os.unlink(temp_filename)\n","\n","    except Exception as e:\n","        # Handle all possible errors (e.g. curl not installed, timeout)\n","        # (Xử lý các lỗi có thể xảy ra như curl chưa cài hoặc hết thời gian)\n","        print(f\"Curl method failed for {url}: {e}\")\n","\n","    # Return None if download failed\n","    # (Trả về None nếu không tải được nội dung)\n","    return None\n","\n","\n","def get_content_enhanced(url):\n","    \"\"\"\n","    Combined method — try requests first, then fall back to cURL.\n","    (Phương thức kết hợp — thử requests trước, nếu thất bại thì dùng cURL.)\n","    \"\"\"\n","    # TODO: Try to fetch content using the main requests-based method\n","    # TODO: (Thử tải nội dung bằng hàm chính sử dụng requests)\n","    content = TODO\n","    if content:\n","        return content  # Return immediately if successful (Trả về ngay nếu thành công)\n","\n","    # TODO: If requests failed, try using cURL as a backup\n","    # (Nếu requests thất bại, dùng cURL làm phương án dự phòng)\n","    # TODO: Nếu requests thất bại, dùng cURL làm backup (bypass anti-bot tốt hơn)\n","    print(f\" Trying curl method for {url[:50]}... (Thử phương pháp curl cho {url[:50]}...)\")\n","    content = TODO\n","    if content:\n","        print(f\" Curl success! (Curl thành công!)\")\n","        return content\n","\n","    # If both methods fail, return None\n","    # (Nếu cả hai phương pháp đều thất bại, trả về None)\n","    return None\n"]},{"cell_type":"markdown","id":"95fb05bc","metadata":{"id":"95fb05bc"},"source":["### Hàm trích xuất nội dung bài báo  \n","### Function to Extract Article Content  \n","\n","**Mục đích**: Phân tích HTML của một bài báo và trích xuất thông tin quan trọng  \n","**Purpose**: Parse the HTML of a news article and extract key information  \n","\n","**Đầu vào**: URL của bài báo VNExpress  \n","**Input**: URL of the VNExpress article  \n","\n","**Đầu ra**: Dictionary chứa:  \n","**Output**: A dictionary containing:  \n","- `title`: Tiêu đề bài báo  \n","  `title`: The title of the article  \n","- `description`: Mô tả ngắn (lead paragraph)  \n","  `description`: Short summary (lead paragraph)  \n","- `contents`: Nội dung đầy đủ  \n","  `contents`: Full article content  \n","- `url`: URL gốc  \n","  `url`: Original article URL  \n","\n","**Kỹ thuật trích xuất**:  \n","**Extraction Techniques**:  \n","1. **Multiple selectors**: Thử nhiều CSS selector khác nhau để tìm content  \n","   **Multiple selectors**: Try various CSS selectors to locate the content  \n","2. **Content filtering**: Loại bỏ các đoạn text không mong muốn (quảng cáo, tags, etc.)  \n","   **Content filtering**: Remove unwanted text such as ads, tags, or footers  \n","3. **Fallback strategy**: Nếu không tìm được content chính, thử các phương pháp khác  \n","   **Fallback strategy**: If main content is not found, try alternative extraction methods  \n","\n"]},{"cell_type":"code","execution_count":null,"id":"3b4942db","metadata":{"id":"3b4942db"},"outputs":[],"source":["def get_content_news_from_news_url(url: str) -> dict:\n","    \"\"\"\n","    Get detailed content of a news article from its URL.\n","    (Lấy nội dung chi tiết của một bài báo từ URL.)\n","    \"\"\"\n","    try:\n","        # Try to get HTML from the URL (automatically chooses between requests and curl)\n","        # (Thử lấy HTML từ URL — hàm này tự động chọn giữa requests hoặc curl tùy trường hợp)\n","        raw_content = get_content_enhanced(url)\n","\n","        # If unable to retrieve content (None returned), stop here\n","        # (Nếu không lấy được nội dung (trả về None), dừng lại và không xử lý thêm)\n","        if raw_content is None:\n","            return None\n","\n","        # If HTML is fetched, parse and extract article info\n","        # (Nếu tải được HTML thành công, chuyển sang bước phân tích và trích xuất thông tin bài viết)\n","        return parse_article_from_html(raw_content, url)\n","\n","    except Exception as e:\n","        # If any unexpected error occurs (network, parsing, syntax, etc.)\n","        # (Nếu xảy ra lỗi bất ngờ như mạng, cú pháp HTML lỗi, hoặc lỗi phân tích)\n","        return None\n","\n","\n","def parse_article_from_html(html_content: str, url: str) -> dict:\n","    \"\"\"\n","    Parse article details from HTML content.\n","    Returns a dict with title, description, content, and URL.\n","    (Phân tích nội dung bài báo từ HTML. Trả về dict gồm tiêu đề, mô tả, nội dung và URL.)\n","    \"\"\"\n","    try:\n","        # Check if HTML is empty or too short (<500 chars)\n","        # (Kiểm tra nếu HTML trống hoặc quá ngắn, có thể là trang lỗi hoặc redirect)\n","        if not html_content or len(html_content) < 500:\n","            return None\n","\n","        # Create BeautifulSoup object for easier HTML navigation\n","        # (Tạo đối tượng BeautifulSoup để dễ dàng duyệt cây HTML và tìm thẻ)\n","        soup = BeautifulSoup(html_content, 'html.parser')\n","\n","        # ===================== EXTRACT TITLE =====================\n","        # (===================== LẤY TIÊU ĐỀ =====================)\n","        title_detail = None\n","\n","        # List of CSS selectors that may contain the title (depends on website structure)\n","        # (Danh sách các selector có thể chứa tiêu đề, vì mỗi chuyên mục VnExpress có cấu trúc HTML hơi khác nhau)\n","        title_selectors = [\n","            \"h1.title-detail\",\n","            \"h1.title_news_detail\",\n","            \"h1[class*='title']\",\n","            \"h1\",\n","            \".title-detail\",\n","            \".title_news_detail\",\n","            \"title\"  # Dự phòng: nếu không tìm thấy trong body, lấy từ thẻ <title> trên <head>\n","        ]\n","\n","        # Iterate through selectors until a valid title is found\n","        # (Duyệt qua từng selector cho đến khi tìm được tiêu đề phù hợp)\n","        for selector in title_selectors:\n","            elements = soup.select(selector)\n","            for element in elements:\n","                text = element.get_text().strip()\n","                # Skip if title is too short or contains site name\n","                # (Bỏ qua nếu tiêu đề quá ngắn hoặc chứa tên trang “vnexpress”)\n","                if text and len(text) > 10 and 'vnexpress' not in text.lower():\n","                    title_detail = text\n","                    break\n","            if title_detail:\n","                break\n","\n","        # If still no title found, skip article\n","        # (Nếu không tìm thấy tiêu đề nào hợp lệ → bỏ qua bài viết này)\n","        if not title_detail:\n","            return None\n","\n","\n","        # ===================== EXTRACT DESCRIPTION =====================\n","        # (===================== LẤY MÔ TẢ =====================)\n","        description = \"\"\n","\n","        # List of possible selectors for article description\n","        # (Danh sách selector có thể chứa đoạn mô tả đầu bài – thường là phần in nghiêng dưới tiêu đề)\n","        desc_selectors = [\n","            \"p.description\",\n","            \"p.lead\",\n","            \"p[class*='description']\",\n","            \"p[class*='lead']\",\n","            \".description\",\n","            \".lead\",\n","            \"meta[name='description']\",           # Nếu không có trong body thì thử lấy từ <meta>\n","            \"meta[property='og:description']\"     # Dự phòng thêm cho mạng xã hội (Open Graph)\n","        ]\n","\n","        # Try each selector until found\n","        # (Thử từng selector cho đến khi tìm thấy đoạn mô tả hợp lệ)\n","        for selector in desc_selectors:\n","            elements = soup.select(selector)\n","            for element in elements:\n","                if element.name == 'meta':\n","                    text = element.get('content', '').strip()\n","                else:\n","                    text = element.get_text().strip()\n","                # Accept only if text length is meaningful\n","                # (Chỉ chấp nhận nếu độ dài mô tả đủ lớn)\n","                if text and len(text) > 20:\n","                    description = text\n","                    break\n","            if description:\n","                break\n","\n","\n","        # ===================== EXTRACT MAIN CONTENT =====================\n","        # (===================== LẤY NỘI DUNG CHÍNH =====================)\n","        content_text = \"\"\n","\n","        # List of possible content containers\n","        # (Danh sách các vùng HTML có thể chứa nội dung bài báo, thử nhiều khả năng khác nhau)\n","        content_selectors = [\n","            \"article.fck_detail p.Normal\",\n","            \"article.fck_detail p\",\n","            \".fck_detail p.Normal\",\n","            \".fck_detail p\",\n","            \"article.content_detail p\",\n","            \".content_detail p\",\n","            \"div.content_detail p\",\n","            \"article p\",\n","            \".Normal\",\n","            \"div[class*='content'] p\",\n","            \"div[class*='article'] p\",\n","            \"main p\"\n","        ]\n","\n","        # Loop through each pattern to extract paragraphs\n","        # (Duyệt qua từng mẫu cấu trúc để gom các đoạn văn thành nội dung hoàn chỉnh)\n","        for selector in content_selectors:\n","            paragraphs = soup.select(selector)\n","            if paragraphs:\n","                temp_content = \"\"\n","                for p in paragraphs:\n","                    text = p.get_text().strip()\n","\n","                    # Skip irrelevant lines such as “Theo VnExpress”, “Ảnh:”, etc.\n","                    # (Bỏ qua các dòng không phải nội dung chính như “Theo VnExpress”, “Ảnh:”, “Video:”)\n","                    if (text and len(text) > 30 and\n","                        not re.match(r'^[\\s\\W]*$', text) and\n","                        not any(keyword in text.lower() for keyword in\n","                               ['theo ', 'nguồn:', 'ảnh:', 'video:', 'xem thêm:', 'tags:',\n","                                'bình luận', 'chia sẻ', 'facebook', 'twitter'])):\n","                        temp_content += text + \" \"\n","\n","                # Accept if content length > 200 chars\n","                # (Chấp nhận nếu tổng độ dài nội dung > 200 ký tự – tránh lấy nhầm phần rác)\n","                if len(temp_content) > 200:\n","                    content_text = temp_content.strip()\n","                    break\n","\n","\n","        # ===================== IF STILL NO CONTENT FOUND =====================\n","        # (===================== NẾU VẪN CHƯA TÌM THẤY NỘI DUNG =====================)\n","        if len(content_text) < 200:\n","            main_content_selectors = [\n","                \"div[class*='content']\",\n","                \"div[class*='article']\",\n","                \"div[class*='detail']\",\n","                \"main\",\n","                \"article\"\n","            ]\n","\n","            # Try to extract text from larger containers\n","            # (Thử lấy toàn bộ nội dung từ các khối lớn hơn nếu không có đoạn <p> rõ ràng)\n","            for selector in main_content_selectors:\n","                containers = soup.select(selector)\n","                for container in containers:\n","                    paragraphs = container.find_all('p')\n","                    temp_content = \"\"\n","                    for p in paragraphs:\n","                        text = p.get_text().strip()\n","                        if text and len(text) > 30:\n","                            temp_content += text + \" \"\n","                    if len(temp_content) > len(content_text):\n","                        content_text = temp_content.strip()\n","                if len(content_text) > 200:\n","                    break\n","\n","\n","        # ===================== FINAL CHECK AND RETURN =====================\n","        # (===================== KIỂM TRA CUỐI CÙNG VÀ TRẢ KẾT QUẢ =====================)\n","        if len(content_text) < 100:\n","            # If article body is too short → likely not a valid article\n","            # (Nếu nội dung quá ngắn → có thể là trang rỗng, hoặc chỉ có video)\n","            return None\n","\n","        # Return final result as dictionary\n","        # (Trả kết quả cuối cùng dưới dạng dictionary để dễ xử lý tiếp)\n","        return {\n","            \"title\": title_detail,\n","            \"description\": description if description else title_detail[:200],\n","            \"contents\": content_text,\n","            \"url\": url\n","        }\n","\n","    except Exception as e:\n","        # If parsing fails, return None\n","        # (Nếu xảy ra lỗi trong quá trình phân tích, trả về None để bỏ qua bài lỗi)\n","        return None\n"]},{"cell_type":"markdown","id":"bed3ecad","metadata":{"id":"bed3ecad"},"source":["### Hàm lấy danh sách links bài báo  \n","### Function to Extract Article Links  \n","\n","**Mục đích**: Từ một trang chủ đề (ví dụ: trang Khoa học), lấy links đến tất cả các bài báo  \n","**Purpose**: From a topic page (e.g., *Science*), extract all article links.  \n","\n","**Quy trình hoạt động**:  \n","**Workflow**:  \n","1. Lấy HTML của trang chủ đề  \n","   **Fetch the HTML** of the topic page.  \n","2. Tìm tất cả thẻ `<a>` có href chứa `.html`  \n","   **Find all `<a>` tags** whose `href` attribute contains `.html`.  \n","3. Lọc chỉ giữ links bài báo VNExpress  \n","   **Filter** to keep only valid VNExpress article links.  \n","4. Loại bỏ duplicates và giới hạn số lượng  \n","   **Remove duplicates** and **limit** the number of links.  \n","\n","**Đầu vào**: URL trang chủ đề (vd: https://vnexpress.net/khoa-hoc)  \n","**Input**: Topic page URL (e.g., https://vnexpress.net/khoa-hoc)  \n","\n","**Đầu ra**: List các URL bài báo (giới hạn 2 links để demo nhanh, trong thực tế có thể tăng lên)  \n","**Output**: A list of article URLs (limited to 2 for quick demo; can be increased in practice).  \n","\n","> **Tip**: Thử in ra một vài links đầu tiên để kiểm tra kết quả trước khi crawl hết!  \n","> **Tip**: Try printing the first few links to verify results before crawling all pages!\n"]},{"cell_type":"code","execution_count":null,"id":"926562e0","metadata":{"id":"926562e0"},"outputs":[],"source":["def get_news_links_from_sub_topic_page_link(sub_topic_page_link: str) -> list:\n","    \"\"\"\n","    Get all article links from a VnExpress topic page.\n","    (Lấy tất cả các liên kết bài báo từ một trang chủ đề của VnExpress.)\n","\n","    Example: From https://vnexpress.net/thoi-su → extract links of individual articles.\n","    (Ví dụ: từ https://vnexpress.net/thoi-su → tìm ra link của các bài viết con.)\n","    \"\"\"\n","\n","    links = []  # List to store all collected article links (Danh sách để lưu tất cả các link bài báo thu thập được)\n","\n","    # Fetch full HTML content from the topic page (like \"Thời sự\", \"Thế giới\", etc.)\n","    # (Gọi hàm lấy toàn bộ HTML từ trang chủ đề, ví dụ: \"Thời sự\", \"Thế giới\", ...)\n","    raw_content = get_content_enhanced(sub_topic_page_link)\n","\n","    # If no HTML content is retrieved, return an empty list\n","    # (Nếu không tải được nội dung HTML thì trả về danh sách rỗng)\n","    if raw_content is None:\n","        return links\n","\n","    # Parse the HTML content using BeautifulSoup for easier tag navigation\n","    # (Phân tích nội dung HTML bằng BeautifulSoup để dễ tìm thẻ)\n","    soup = BeautifulSoup(raw_content, 'html.parser')\n","\n","    # ===================== LIST OF POSSIBLE SELECTORS =====================\n","    # (===================== DANH SÁCH CÁC SELECTOR =====================)\n","    # Websites can use different HTML structures for articles.\n","    # (Mỗi website có thể có cấu trúc HTML khác nhau cho bài báo.)\n","    # Example: Articles may be inside <article>, <h2>, or <h3> tags.\n","    # (Ví dụ: bài báo có thể nằm trong <article>, hoặc chỉ có <h2>/<h3> chứa thẻ <a>.)\n","    link_selectors = [\n","        \"article h2.title-news a\",  # Most common form (Dạng phổ biến nhất)\n","        \"article h3.title-news a\",  # Some pages use <h3> instead of <h2> (Một số trang dùng h3)\n","        \"h2.title-news a\",          # Without <article> wrapper (Không có thẻ <article> bọc ngoài)\n","        \"h3.title-news a\",\n","        \"h2 a[href*='.html']\",      # Any <h2> with an href containing \".html\" (Bất kỳ thẻ h2 có .html)\n","        \"h3 a[href*='.html']\",\n","        \"article a[href*='.html']\", # All <a> inside <article> that link to .html (Mọi link trong <article> có .html)\n","        \"a[href*='.html']\"          # Fallback: any <a> with .html (Dự phòng: mọi <a> có .html)\n","    ]\n","\n","    # Base URL to complete relative links\n","    # (URL gốc của trang, dùng để ghép nếu link thiếu phần đầu)\n","    base_url = \"https://vnexpress.net\"\n","\n","    # ===================== LOOP THROUGH SELECTORS =====================\n","    # (===================== DUYỆT QUA CÁC SELECTOR =====================)\n","    for selector in link_selectors:\n","        # TODO: Find all <a> tags matching the current selector\n","        # TODO: (Tìm tất cả thẻ <a> phù hợp với selector hiện tại)\n","        link_elements = TODO\n","\n","        for element in link_elements:\n","            # Extract the \"href\" attribute value\n","            # (Lấy giá trị thuộc tính href)\n","            href = element.get('href')\n","\n","            if href:\n","                # ------------------- CLEAN URL -------------------\n","                # (------------------- LÀM SẠCH URL -------------------)\n","                # Remove fragment part after \"#\" if present\n","                # (Nếu URL có dấu #, chỉ giữ phần trước)\n","                if '#' in href:\n","                    href = href.split('#')[0]\n","\n","                # ------------------- NORMALIZE PATH -------------------\n","                # (------------------- CHUẨN HÓA ĐƯỜNG DẪN -------------------)\n","                # If relative path (starts with \"/\"), join with base_url\n","                # (Nếu link bắt đầu bằng / → nối thêm base_url)\n","                if href.startswith('/'):\n","                    href = base_url + href\n","                # Skip if link doesn’t start with \"http\" (invalid or external format)\n","                # (Bỏ qua nếu link không bắt đầu bằng http)\n","                elif not href.startswith('http'):\n","                    continue\n","\n","                # ------------------- VALIDATION CHECKS -------------------\n","                # (------------------- KIỂM TRA TÍNH HỢP LỆ -------------------)\n","                # Conditions:\n","                # - Must contain \".html\" (Phải có .html)\n","                # - Must include at least one digit (Có ít nhất 1 chữ số)\n","                # - Must contain \"vnexpress.net\" (Là link nội bộ)\n","                # - Must be longer than 50 chars (Dài hơn 50 ký tự)\n","                if ('.html' in href and\n","                    re.search(r'\\d+', href) and\n","                    'vnexpress.net' in href and\n","                    len(href) > 50):\n","                    links.append(href)\n","\n","        # Stop if valid links are found (don’t test other selectors)\n","        # (Nếu đã tìm thấy link hợp lệ thì dừng, không thử selector khác)\n","        if links:\n","            break\n","\n","    # ===================== POST-PROCESSING =====================\n","    # (===================== XỬ LÝ KẾT QUẢ =====================)\n","    # Remove duplicate links using set, then convert back to list\n","    # (Loại bỏ các link trùng lặp bằng set rồi chuyển lại thành list)\n","    unique_links = list(set(links))\n","\n","    # Limit result to first 2 links to prevent overloading\n","    # (Giới hạn chỉ lấy 2 link đầu tiên để tránh tải quá nhiều)\n","    return unique_links[:2]\n"]},{"cell_type":"markdown","id":"45519073","metadata":{"id":"45519073"},"source":["### Hàm xử lý phân trang  \n","### Function for Pagination Handling  \n","\n","**Mục đích**: Tạo URLs cho nhiều trang của cùng một chủ đề  \n","**Purpose**: Generate URLs for multiple pages of the same topic.  \n","\n","VNExpress có cấu trúc phân trang như sau:  \n","VNExpress uses the following pagination structure:  \n","- Trang 1: `https://vnexpress.net/khoa-hoc`  \n","  → Page 1: `https://vnexpress.net/khoa-hoc`  \n","- Trang 2: `https://vnexpress.net/khoa-hoc-p2`  \n","  → Page 2: `https://vnexpress.net/khoa-hoc-p2`  \n","- Trang 3: `https://vnexpress.net/khoa-hoc-p3`  \n","  → Page 3: `https://vnexpress.net/khoa-hoc-p3`  \n","\n","**Đầu vào**:  \n","**Input**:  \n","- `sub_topic_url`: URL trang đầu tiên  \n","  → The URL of the first page.  \n","- `pages`: Số trang muốn crawl  \n","  → The number of pages to crawl.  \n","\n","**Đầu ra**: List các URL của tất cả các trang  \n","**Output**: A list of all page URLs.  \n","\n","> **Thử nghiệm**: Thay đổi số trang từ 1 lên 3 để lấy nhiều bài báo hơn (nhưng cũng tốn thời gian hơn)!  \n","> **Experiment**: Try changing the number of pages from 1 to 3 to collect more articles (but it will take longer)!\n"]},{"cell_type":"code","execution_count":null,"id":"52d99a82","metadata":{"id":"52d99a82"},"outputs":[],"source":["def get_page_urls_from_sub_topic_url(sub_topic_url, pages=1) -> list:\n","    \"\"\"\n","    Generate a list of paginated URLs for a given VnExpress sub-topic.\n","    (Tạo danh sách các URL có phân trang cho một chuyên mục của VnExpress.)\n","\n","    Example (Ví dụ):\n","        sub_topic_url = \"https://vnexpress.net/the-gioi\"\n","        pages = 3\n","        Output (Kết quả):\n","        [\n","            \"https://vnexpress.net/the-gioi\",\n","            \"https://vnexpress.net/the-gioi-p2\",\n","            \"https://vnexpress.net/the-gioi-p3\"\n","        ]\n","    \"\"\"\n","\n","    # Initialize the list with the base URL (first page)\n","    # (Khởi tạo danh sách URL với trang đầu tiên — trang gốc của chuyên mục)\n","    urls = [sub_topic_url]\n","\n","    # Loop through page numbers from 2 to the specified number of pages\n","    # (Lặp qua các số trang từ 2 đến giá trị `pages` được truyền vào)\n","    for page in range(2, pages + 1):\n","\n","        # Append the suffix \"-p{page}\" to generate subsequent page URLs\n","        # (Ghép thêm hậu tố \"-p{page}\" để tạo ra URL cho các trang tiếp theo)\n","        #\n","        # Ghi chú:\n","        #   - Cấu trúc URL phân trang của VnExpress luôn tuân theo mẫu này:\n","        #       /chuyen-muc      → Trang 1\n","        #       /chuyen-muc-p2   → Trang 2\n","        #       /chuyen-muc-p3   → Trang 3\n","        #   - Vì vậy, chỉ cần nối chuỗi đơn giản là đủ.\n","        new_url = f\"{sub_topic_url}-p{page}\"\n","\n","        # Add this generated URL to the list\n","        # (Thêm URL vừa tạo vào danh sách)\n","        urls.append(new_url)\n","\n","    # Return the complete list of URLs\n","    # (Trả về toàn bộ danh sách các URL phân trang)\n","    return urls\n"]},{"cell_type":"markdown","id":"b55f272f","metadata":{"id":"b55f272f"},"source":["### Hàm tổng hợp - Crawl tất cả chủ đề  \n","### Aggregated Function – Crawl All Topics  \n","\n","**Mục đích**: Hàm chính để crawl links bài báo từ nhiều chủ đề khác nhau  \n","**Purpose**: The main function to crawl article links from multiple topics.  \n","\n","**Quy trình hoạt động**:  \n","**How It Works:**  \n","1. Duyệt qua từng chủ đề trong danh sách  \n","   → Iterate through each topic in the list  \n","2. Với mỗi chủ đề, lấy links của các trang (phân trang)  \n","   → For each topic, get links from its paginated pages  \n","3. Với mỗi trang, lấy links của các bài báo  \n","   → For each page, extract the article links  \n","4. Loại bỏ duplicates và tổng hợp kết quả  \n","   → Remove duplicates and combine all results  \n","\n","**Đầu vào**:  \n","**Input:**  \n","- `topics_links`: Dictionary chứa các chủ đề và sub-topics  \n","  → A dictionary containing topics and their sub-topics  \n","- `n_pages_per_topic`: Số trang crawl cho mỗi chủ đề  \n","  → Number of pages to crawl for each topic  \n","\n","**Đầu ra**: Dictionary với structure tương tự đầu vào nhưng chứa links bài báo  \n","**Output:** A dictionary with the same structure as input but containing article links.  \n","\n","**Tính năng đặc biệt**:  \n","**Special Features:**  \n","- Hiển thị progress bar với `tqdm`  \n","  → Display progress bar using `tqdm`  \n","- Tự động delay giữa các request  \n","  → Automatically adds delay between requests  \n","- In thống kê số lượng bài báo tìm được  \n","  → Prints a summary of how many articles were found  \n","\n","> **Lưu ý về thời gian**: Crawl nhiều chủ đề và nhiều trang sẽ mất thời gian. Hãy bắt đầu với ít chủ đề để test!  \n","> **Time Note**: Crawling many topics and pages takes time — start small for testing!\n"]},{"cell_type":"code","execution_count":null,"id":"145b0af9","metadata":{"id":"145b0af9"},"outputs":[],"source":["def get_all_news_urls_from_topics_links(topics_links: dict, n_pages_per_topic=1) -> dict:\n","    \"\"\"\n","    Collect all news article URLs from a list of topic links.\n","    (Lấy toàn bộ link bài báo từ danh sách các chủ đề (topics_links).)\n","\n","    Each main topic may contain several sub-topics,\n","    and each sub-topic may have multiple paginated pages.\n","    (Mỗi chủ đề chính có thể gồm nhiều chủ đề con,\n","    và mỗi chủ đề con có thể có nhiều trang phân trang.)\n","    \"\"\"\n","\n","    # Initialize the output dictionary to store final results\n","    # (Khởi tạo dict để lưu kết quả cuối cùng)\n","    # Format (Định dạng):\n","    # {\n","    #   \"Thời sự\": [\"link1\", \"link2\", ...],\n","    #   \"Thế giới\": [\"link1\", \"link2\", ...],\n","    #   ...\n","    # }\n","    output = {}\n","\n","    # Iterate through each main topic in the dictionary\n","    # (Duyệt qua từng chủ đề chính trong từ điển topics_links)\n","    # k = topic name (tên chủ đề), v = list of sub-topic URLs (danh sách các link chủ đề con)\n","    for k, v in tqdm(topics_links.items(), desc=\"Processing topics (Xử lý các chủ đề)\"):\n","        print(f'\\n Topic: {k} - Subtopics: {len(v)} (Số chủ đề con: {len(v)})')\n","\n","        # Create an empty list to store all article links for this topic\n","        # (Tạo danh sách rỗng để lưu các link bài báo của chủ đề hiện tại)\n","        output[k] = []\n","\n","        # Temporary list to store all paginated sub-topic page URLs\n","        # (Tạo danh sách tạm để lưu tất cả link các trang phân trang của chủ đề con)\n","        page_links = []\n","\n","        # ------------------------------------------------------------\n","        # STEP 1: Generate all paginated page URLs for each sub-topic\n","        # (BƯỚC 1: Tạo danh sách link các trang (phân trang) cho từng chủ đề con)\n","        # ------------------------------------------------------------\n","        for sub_topic_link in tqdm(v, desc=f\"Generating page links for {k} (Tạo link trang cho {k})\", leave=False):\n","            # Example:\n","            # https://vnexpress.net/the-gioi → https://vnexpress.net/the-gioi-p2 → https://vnexpress.net/the-gioi-p3\n","            # (Ví dụ cách tạo các link phân trang từ link chủ đề con)\n","            # TODO: Create a list of links for paginated pages (e.g. -p2, -p3)\n","            # Hint: use the function get_page_urls_from_sub_topic_url(sub_topic_link, pages=n_pages_per_topic)\n","            s = TODO\n","            # Add all generated page links into the main list\n","            # (Thêm toàn bộ các link trang vừa tạo vào danh sách page_links)\n","            page_links.extend(s)\n","\n","        # ------------------------------------------------------------\n","        # STEP 2: From each page, extract all article links\n","        # (BƯỚC 2: Từ mỗi trang, lấy các link bài báo cụ thể)\n","        # ------------------------------------------------------------\n","        for page_link in tqdm(page_links, desc=f\"Extracting article links for {k} (Lấy link bài báo cho {k})\", leave=False):\n","            # Call the function that extracts article URLs from a sub-topic page\n","            # (Gọi hàm lấy link bài báo từ trang con)\n","            news_links = get_news_links_from_sub_topic_page_link(page_link)\n","\n","            # Add all extracted article links into the topic’s list\n","            # (Thêm toàn bộ link bài báo lấy được vào danh sách của chủ đề)\n","            output[k].extend(news_links)\n","\n","            # Wait 1 second between each request to avoid IP ban or server blocking\n","            # (Tạm nghỉ 1 giây giữa các lần gọi để tránh bị server chặn hoặc giới hạn tốc độ)\n","            time.sleep(1)\n","\n","        # ------------------------------------------------------------\n","        # STEP 3: Remove duplicate links\n","        # (BƯỚC 3: Loại bỏ các link trùng lặp)\n","        # ------------------------------------------------------------\n","        output[k] = list(set(output[k]))  # Convert list → set → list again to remove duplicates\n","        print(f\" Found {len(output[k])} unique articles for {k} (Tìm được {len(output[k])} bài báo duy nhất cho {k})\")\n","\n","    # Return the final dictionary containing all collected article links\n","    # (Trả về toàn bộ kết quả cuối cùng dưới dạng từ điển)\n","    return output\n"]},{"cell_type":"markdown","id":"6d009211","metadata":{"id":"6d009211"},"source":["# Cấu hình dữ liệu cần crawl  \n","# Data Configuration for Crawling  \n","\n","## Thiết lập các chủ đề cần thu thập  \n","## Define the Topics to Be Collected  \n","\n","Trong bước này, chúng ta sẽ cấu hình những chủ đề tin tức muốn crawl từ VNExpress.  \n","In this step, we will configure which news topics to crawl from VNExpress.  \n","\n","Mỗi chủ đề có thể có nhiều sub-topic (chủ đề con).  \n","Each topic may contain several sub-topics.  \n","\n","### Cấu trúc dữ liệu:  \n","### Data Structure:  \n","```python\n","topics_links = {\n","    'tên_chủ_đề': [               # topic name\n","        'url_sub_topic_1',        # URL of sub-topic 1\n","        'url_sub_topic_2',        # URL of sub-topic 2\n","        ...\n","    ]\n","}\n","```\n","###  Lời khuyên cho việc chọn chủ đề:\n","###  Tips for Choosing Topics:\n","- **Bắt đầu nhỏ** : Chỉ chọn 2-3 chủ đề cho lần chạy đầu tiên\n","\n","- **Start small** : Choose only 2–3 topics for your first run\n","- **Đa dạng nội dung**: Chọn các chủ đề khác nhau để có dataset phong phú\n","- **Diversify content**: Choose varied topics for a richer dataset\n","- **Kiểm tra URL**: Đảm bảo các URL có thể truy cập được\n","- **Check URLs**: Make sure the URLs are accessible\n","\n","> **Quan trọng**: Trong demo này chúng ta chỉ lấy 1 sub-topic của mỗi chủ đề để tiết kiệm thời gian. Trong thực tế, bạn có thể uncomment các dòng khác để lấy nhiều hơn.\n","> **Important**: In this demo, we only take 1 sub-topic per topic to save time.\n","Trong thực tế, bạn có thể uncomment các dòng khác để lấy nhiều hơn.\n","In practice, you can uncomment other lines to collect more."]},{"cell_type":"code","execution_count":null,"id":"b6b59696","metadata":{"id":"b6b59696"},"outputs":[],"source":["# Cấu hình các chủ đề - giảm số lượng để test\n","# Configure the news topics - reduce the number for testing\n","\n","topics_links = {\n","    'khoa-hoc': [  # Chủ đề: Khoa học (Science)\n","        # 'https://vnexpress.net/khoa-hoc/tin-tuc',  # (Bỏ để test nhanh)\n","        'https://vnexpress.net/khoa-hoc/phat-minh'   # Đường dẫn đến mục “Phát minh” (Invention)\n","    ],\n","    'the-thao': [  # Chủ đề: Thể thao (Sports)\n","        # 'https://vnexpress.net/bong-da',            # (Bỏ để test nhanh)\n","        'https://vnexpress.net/the-thao/tennis'       # Đường dẫn đến mục “Tennis”\n","    ],\n","    # 'doi-song': [                                   # Một chủ đề khác: Đời sống (Life)\n","    #     # 'https://vnexpress.net/doi-song/to-am',   # (Tạm ẩn để giảm tải khi test)\n","    #     'https://vnexpress.net/doi-song/bai-hoc-song' # Mục “Bài học sống” (Life lessons)\n","    # ]\n","}\n","\n","# In ra màn hình danh sách các chủ đề đã được cấu hình\n","# Print out the list of configured topics\n","\n","print(\" Themes configured:\")  # Notify that the topics have been set (Thông báo đã cấu hình xong)\n","for topic, links in topics_links.items():\n","    # Duyệt qua từng cặp key-value: topic (tên chủ đề), links (danh sách các liên kết con)\n","    # Iterate through each key-value pair: topic name and list of subtopic links\n","    print(f\"- {topic}: {len(links)} sub-topic\")\n","    # In ra tên chủ đề và số lượng chủ đề con của nó\n","    # Print the topic name and the number of its subtopics\n"]},{"cell_type":"markdown","id":"06ab07d1","metadata":{"id":"06ab07d1"},"source":["## Test hệ thống crawl (Bước quan trọng!)\n","## Test the crawling system (Important step!)\n","\n","Trước khi crawl hàng loạt, chúng ta **PHẢI** test hệ thống để đảm bảo tất cả đều hoạt động tốt.  \n","Before doing a large-scale crawl, we **MUST** test the system to ensure everything works properly.  \n","\n","Đây là thực hành tốt nhất trong web crawling.  \n","This is a best practice in web crawling.  \n","\n","---\n","\n","###  Test này sẽ kiểm tra:\n","###  This test will check:\n","\n","1. **Kết nối mạng**: Có thể truy cập VNExpress không?  \n","   **Network connection**: Can we access VNExpress?\n","\n","2. **Bypass anti-bot**: Các kỹ thuật chống phát hiện có hiệu quả không?  \n","   **Bypass anti-bot**: Are the anti-detection techniques effective?\n","\n","3. **Parsing HTML**: Có trích xuất được nội dung không?  \n","   **Parsing HTML**: Can we extract the content correctly?\n","\n","4. **Error handling**: Xử lý lỗi có ổn định không?  \n","   **Error handling**: Is the error handling stable?\n","\n","---\n","\n","###  Kết quả mong đợi:\n","###  Expected results:\n","\n","- **Tìm được links**: Ít nhất 10–20 links bài báo từ trang test  \n","  **Find links**: At least 10–20 article links from the test page  \n","\n","- **Crawl thành công**: Ít nhất 2/3 bài báo test được crawl thành công  \n","  **Successful crawl**: At least 2/3 of the test articles are crawled successfully  \n","\n","- **Nội dung đầy đủ**: Tiêu đề + nội dung có độ dài hợp lý  \n","  **Complete content**: Title + body have reasonable length  \n","\n","---\n","\n",">  **Nếu test thất bại**: Đừng tiếp tục!  \n",">  **If the test fails**: Do not continue!  \n","\n","> Hãy kiểm tra kết nối mạng hoặc thử lại sau 10–15 phút vì có thể website đang bảo trì.  \n","> Check your internet connection or try again after 10–15 minutes — the website might be under maintenance.\n"]},{"cell_type":"code","execution_count":null,"id":"0619699e","metadata":{"id":"0619699e"},"outputs":[],"source":["# Test detailed debug with the new methods (requests + curl)\n","# (Test/Debug chi tiết với các phương pháp mới (requests + curl))\n","test_url = 'https://vnexpress.net/khoa-hoc/tin-tuc'\n","\n","# Print the URL being tested\n","# (In URL đang test)\n","print(f'Testing link extraction from: {test_url} (Test lấy links từ: {test_url})')\n","# Separator line for readability\n","# (Dòng phân cách để dễ nhìn)\n","print(\"─\" * 60)\n","\n","# Announce the testing approach (combined methods)\n","# (Thông báo phương pháp test: kết hợp requests + curl)\n","print(\"Testing combined method (requests + curl)... (Test phương pháp kết hợp (requests + curl)...)\")\n","\n","# Call the function to extract article links from a topic page\n","# (Gọi hàm để lấy các link bài báo từ trang chủ đề)\n","test_links = get_news_links_from_sub_topic_page_link(test_url)\n","\n","# Print how many links were found\n","# (In ra số lượng link tìm được)\n","print(f'Found {len(test_links)} links (Tìm thấy {len(test_links)} links)')\n","\n","# If any links were found, perform further checks\n","# (Nếu tìm thấy link thì thực hiện kiểm tra tiếp)\n","if test_links:\n","    # Print top 5 found links for quick inspection\n","    # (In 5 link đầu để kiểm tra nhanh)\n","    print('\\nTop 5 links found: (Top 5 links tìm được:)')\n","    for i, link in enumerate(test_links[:5]):  # iterate first 5 items\n","        # Print each link with index\n","        # (In từng link kèm số thứ tự)\n","        print(f'{i+1}. {link}')\n","\n","    # Separator and next test section title\n","    # (Dòng phân cách và tiêu đề phần test tiếp theo)\n","    print('\\n' + \"─\" * 60)\n","    print('Testing article content crawling with enhanced method (requests -> curl)... (Test crawl nội dung bài báo với phương pháp nâng cao (requests -> curl)...)')\n","\n","    # Counter for successful article crawls in this sample test\n","    # (Bộ đếm số bài crawl thành công trong test mẫu này)\n","    success_count = 0\n","\n","    # Try to crawl and parse the first 3 links to verify parsing quality\n","    # (Thử crawl và parse 3 link đầu để kiểm tra chất lượng parsing)\n","    for i, link in enumerate(test_links[:3]):  # limit to first 3 links for the test\n","        # Print a shortened preview of the link for clarity\n","        # (In phần đầu của link để dễ quan sát)\n","        print(f'\\nTest link {i+1}: {link[:80]}... (Test link {i+1}: {link[:80]}...)')\n","\n","        # Fetch and parse the article using the combined method (requests -> curl)\n","        # (Lấy nội dung bài báo bằng phương pháp kết hợp)\n","        article = get_content_news_from_news_url(link)\n","\n","        # If parsing was successful, print summary info and increment success counter\n","        # (Nếu parse thành công, in tóm tắt và tăng biến đếm thành công)\n","        if article:\n","            print('Success! (Thành công!)')\n","            # Print title (short) for verification\n","            # (In tiêu đề (rút gọn) để kiểm tra)\n","            print(f'    Title (Tiêu đề): {article[\"title\"]}')\n","            # Print first 100 chars of description to inspect it\n","            # (In 100 ký tự đầu của mô tả để kiểm tra)\n","            print(f'    Description (Mô tả): {article[\"description\"][:100]}...')\n","            # Print first 150 chars of article body as a preview\n","            # (In 150 ký tự đầu của nội dung bài báo)\n","            print(f'    Content preview (Nội dung - xem trước): {article[\"contents\"][:150]}...')\n","            # Print the length of the content to know its size\n","            # (In độ dài nội dung để biết kích thước)\n","            print(f'    Length: {len(article[\"contents\"])} characters (Độ dài: {len(article[\"contents\"])} ký tự)')\n","            success_count += 1\n","        else:\n","            # If parsing failed, report failure for that link\n","            # (Nếu không lấy được nội dung, báo thất bại cho link đó)\n","            print('Failed (Thất bại)')\n","\n","    # Print summary of the 3-link test\n","    # (In kết quả tổng kết test với 3 link)\n","    print(f'\\nTest result: {success_count}/3 succeeded (Kết quả test: {success_count}/3 thành công)')\n","\n","    # If at least one article parsed successfully, consider system OK to continue\n","    # (Nếu có ít nhất 1 bài thành công → hệ thống có thể tiếp tục crawl)\n","    if success_count > 0:\n","        print('System looks OK, you can continue crawling! (Hệ thống hoạt động tốt, có thể tiếp tục crawl!)')\n","    else:\n","        # If none succeeded, perform deeper debugging on the first link\n","        # (Nếu tất cả thất bại → debug chi tiết link đầu tiên)\n","        print('There is an issue with the crawling system, please check! (Có vấn đề với hệ thống crawl, cần kiểm tra lại!)')\n","\n","        print('\\nDetailed debug for the first link: (Debug chi tiết cho link đầu tiên:)')\n","        # Save the first link for step-by-step testing\n","        # (Lưu link đầu tiên để test từng phương pháp)\n","        first_link = test_links[0]\n","\n","        print(\"Testing each method separately: (Test riêng từng method:)\")\n","\n","        # ------------------ Test requests-based method ------------------\n","        # Test the requests method (get_content_) to see if it can fetch HTML\n","        # (Test phương pháp requests (get_content_) xem có lấy được HTML không)\n","        print(\"Testing requests method: (Test requests method:)\")\n","        raw_html_requests = get_content_(first_link)\n","        if raw_html_requests:\n","            # If requests succeeded, print length of HTML\n","            # (Nếu requests lấy được, in độ dài HTML)\n","            print(f'    Requests OK, length: {len(raw_html_requests)} characters (Requests OK, độ dài: {len(raw_html_requests)} ký tự)')\n","        else:\n","            # Otherwise indicate requests failed for this URL\n","            # (Ngược lại, báo requests failed)\n","            print('    Requests failed (Requests failed)')\n","\n","        # ------------------ Test curl-based method ------------------\n","        # Test the cURL backup method (get_content_with_curl)\n","        # (Test phương pháp dự phòng cURL)\n","        print(\"Testing curl method: (Test curl method:)\")\n","        raw_html_curl = get_content_with_curl(first_link)\n","        if raw_html_curl:\n","            # If curl returned HTML, print length\n","            # (Nếu curl lấy được HTML, in độ dài)\n","            print(f'    Curl OK, length: {len(raw_html_curl)} characters (Curl OK, độ dài: {len(raw_html_curl)} ký tự)')\n","\n","            # Quick structure inspection: parse HTML from curl to check tags\n","            # (Kiểm tra nhanh cấu trúc HTML nhận từ curl để xem có thẻ h1, p,...)\n","            soup = BeautifulSoup(raw_html_curl, 'html.parser')\n","\n","            # Count number of H1 tags (likely headline tags)\n","            # (Đếm số thẻ h1 — thường là thẻ tiêu đề)\n","            h1_tags = soup.find_all('h1')\n","            print(f'      Number of <h1> tags: {len(h1_tags)} (Số thẻ h1: {len(h1_tags)})')\n","\n","            # Print first 3 h1 text samples (trimmed) to inspect headline extraction\n","            # (In 3 thẻ h1 đầu (rút gọn) để kiểm tra việc trích tiêu đề)\n","            for h1 in h1_tags[:3]:\n","                print(f'       - {h1.get_text().strip()[:100]}')\n","\n","            # Count all <p> tags and those with >50 chars (likely article paragraphs)\n","            # (Đếm thẻ <p> tổng và số thẻ có nội dung >50 ký tự — khả năng là đoạn nội dung)\n","            p_tags = soup.find_all('p')\n","            content_p = [p for p in p_tags if len(p.get_text().strip()) > 50]\n","            print(f'      <p> tags with content >50 chars: {len(content_p)}/{len(p_tags)} (Số thẻ p có nội dung: {len(content_p)}/{len(p_tags)})')\n","        else:\n","            # If curl also failed, indicate that as well\n","            # (Nếu curl cũng thất bại, in thông báo)\n","            print('    Curl failed (Curl failed)')\n","\n","        # ------------------ Test combined/enhanced method ------------------\n","        # Test the enhanced method which tries requests first then curl if needed\n","        # (Test phương pháp kết hợp: requests rồi fallback sang curl)\n","        print(\"Testing enhanced method (requests -> curl): (Test enhanced method:)\")\n","        raw_html_enhanced = get_content_enhanced(first_link)\n","        if raw_html_enhanced:\n","            # If enhanced method succeeded, print length\n","            # (Nếu enhanced thành công, in độ dài)\n","            print(f'    Enhanced OK, length: {len(raw_html_enhanced)} characters (Enhanced OK, độ dài: {len(raw_html_enhanced)} ký tự)')\n","        else:\n","            # Otherwise show enhanced failed as well\n","            # (Ngược lại báo enhanced failed)\n","            print('    Enhanced failed (Enhanced failed)')\n","\n","# If no links found on topic page, debug the topic page HTML itself\n","# (Nếu không tìm thấy link nào, debug trang chủ đề để kiểm tra cấu trúc)\n","else:\n","    print('No links found (Không tìm thấy links nào)')\n","    print('Debugging the topic page... (Debug trang chủ đề...)')\n","    # Fetch raw HTML of the topic page for manual inspection\n","    # (Lấy HTML trang chủ để kiểm tra thủ công)\n","    raw_html = get_content_enhanced(test_url)\n","    if raw_html:\n","        # Print the length of the fetched topic page HTML\n","        # (In độ dài HTML của trang chủ đề)\n","        print(f'    Topic page HTML fetched, length: {len(raw_html)} characters (Lấy được HTML trang chủ, độ dài: {len(raw_html)} ký tự)')\n","        soup = BeautifulSoup(raw_html, 'html.parser')\n","\n","        # Find all <a> tags with href attributes and filter those containing \".html\"\n","        # (Tìm tất cả <a href=\"\"> rồi lọc những href chứa \".html\")\n","        all_links = soup.find_all('a', href=True)\n","        html_links = [a['href'] for a in all_links if '.html' in a.get('href', '')]\n","\n","        # Print how many .html links found on the page (quick sanity check)\n","        # (In số link chứa .html tìm thấy trên trang — kiểm tra nhanh)\n","        print(f'Found {len(html_links)} links containing .html (Tìm thấy {len(html_links)} links chứa .html)')\n","        # Print first 5 of these links for inspection\n","        for i, link in enumerate(html_links[:5]):\n","            print(f'   {i+1}. {link}')\n","    else:\n","        # If topic page HTML can't be fetched, report it\n","        # (Nếu không lấy được HTML trang chủ, báo lỗi)\n","        print('Could not fetch topic page HTML (Không lấy được HTML trang chủ)')\n","\n","# Final tips and summary printed to the user\n","# (Lời khuyên/tóm tắt cuối cùng in cho người dùng)\n","print(\"\\n\" + \"─\" * 60)\n","print(\"TIP: If curl works but requests fails, the site may block Python requests but allow curl. (TIP: Nếu curl method thành công mà requests failed, có thể VNExpress chặn Python requests nhưng cho phép curl.)\")\n","print(\"Enhanced method will automatically fallback to curl when needed. (Phương pháp nâng cao sẽ tự động fallback sang curl khi cần.)\")\n"]},{"cell_type":"markdown","id":"f7af8a11","metadata":{"id":"f7af8a11"},"source":["##  Thu thập danh sách links bài báo\n","## Collect the list of article links\n","\n","**Điều kiện**: Chỉ chạy cell này nếu **test ở bước trước đã thành công**!  \n","**Condition**: Only run this cell if the **test in the previous step was successful**!\n","\n","---\n","\n","### Trong bước này, chúng ta sẽ:\n","###  In this step, we will:\n","\n","1. Duyệt qua tất cả các chủ đề đã cấu hình  \n","   Browse through all configured topics  \n","\n","2. Lấy danh sách links bài báo từ mỗi trang chủ đề  \n","   Get the list of article links from each topic page  \n","\n","3. Tổng hợp và đếm số lượng bài báo tìm được  \n","   Combine and count the number of articles found  \n","\n","---\n","\n","###  Thời gian dự kiến:\n","###  Estimated time:\n","\n","- **1 chủ đề, 1 trang**: ~30 giây  \n","  **1 topic, 1 page**: ~30 seconds  \n","\n","- **3 chủ đề, 1 trang mỗi chủ đề**: ~2 phút  \n","  **3 topics, 1 page each**: ~2 minutes  \n","\n","- **3 chủ đề, 3 trang mỗi chủ đề**: ~5–7 phút  \n","  **3 topics, 3 pages each**: ~5–7 minutes  \n","\n","---\n","\n","###  Số lượng bài báo dự kiến:\n","###  Expected number of articles:\n","\n","- Mỗi trang thường có 15–25 bài báo  \n","  Each page usually has 15–25 articles  \n","\n","- Một số bài có thể bị duplicate hoặc không crawl được  \n","  Some articles may be duplicated or fail to crawl  \n","\n","---\n","\n",">  **Mẹo**: Theo dõi output để xem quá trình diễn ra như thế nào.  \n",">  **Tip**: Watch the output to see how the process runs.  \n","\n","> Nếu một chủ đề nào đó mất quá nhiều thời gian, có thể dừng và bỏ chủ đề đó.  \n","> If any topic takes too long, you may stop and skip that topic.\n"]},{"cell_type":"code","execution_count":null,"id":"eaf658de","metadata":{"id":"eaf658de"},"outputs":[],"source":["# Run only if the previous crawl test was successful\n","# (Chỉ chạy nếu phần test crawl trước đó đã thành công)\n","print(' Start collecting article links... (Bắt đầu lấy danh sách links bài báo...)')\n","\n","# Number of pages to fetch per sub-topic (pagination count)\n","# (Số lượng trang muốn lấy trong mỗi chủ đề con - dùng cho phân trang)\n","n_pages_per_topic = 1\n","\n","# Call the main function to collect all article links from topic list\n","# (Gọi hàm chính để lấy toàn bộ danh sách link bài báo từ danh sách chủ đề)\n","# This function will:\n","# (Hàm này sẽ:)\n","#  - Iterate through each main topic in 'topics_links'\n","#    (  - Duyệt qua từng chủ đề chính trong 'topics_links')\n","#  - Get all sub-topic page URLs (pagination)\n","#    (  - Lấy danh sách các trang con (có phân trang))\n","#  - Call get_news_links_from_sub_topic_page_link() to extract actual article URLs\n","#    (  - Gọi hàm get_news_links_from_sub_topic_page_link() để lấy link bài báo thực tế)\n","topics_links_news =  # TODO: Fetch list of article URLs (TODO: Lấy danh sách link bài báo)\n","\n","# After collecting links, print statistics for each topic\n","# (Sau khi lấy xong, in thống kê số lượng bài báo cho từng chủ đề)\n","print('\\n Number of articles per topic: (Số lượng bài báo cho từng chủ đề:)')\n","\n","total_articles = 0  # Counter for total number of collected articles (Biến đếm tổng số bài báo)\n","\n","# Iterate through each topic in the resulting dictionary\n","# (Lặp qua từng chủ đề trong kết quả trả về)\n","for k, v in topics_links_news.items():\n","    # Print topic name and number of articles found\n","    # (In tên chủ đề và số lượng bài báo tìm thấy)\n","    print(f'- {k}: {len(v)} articles (bài báo)')\n","    # Add count to total\n","    # (Cộng dồn vào tổng số bài báo)\n","    total_articles += len(v)\n","\n","# Print total number of articles collected across all topics\n","# (In tổng số bài báo lấy được từ tất cả các chủ đề)\n","print(f'\\n Total: {total_articles} articles collected (Tổng cộng: {total_articles} bài báo đã được lấy)')\n"]},{"cell_type":"markdown","id":"225cc575","metadata":{"id":"225cc575"},"source":["#  Tiền xử lý dữ liệu  \n","#  Data Preprocessing  \n","\n","##  Tạo cấu trúc thư mục lưu trữ  \n","##  Create directory structure for storage  \n","\n","Trước khi bắt đầu crawl nội dung, chúng ta cần tạo các thư mục để tổ chức dữ liệu một cách khoa học.  \n","Before starting to crawl content, we need to create folders to organize data in a structured and logical way.  \n","\n","---\n","\n","###  Mục đích:\n","###  Purpose:\n","\n","- Giúp lưu trữ dữ liệu rõ ràng theo từng chủ đề  \n","  Helps store data clearly by topic  \n","\n","- Dễ dàng truy cập và xử lý về sau  \n","  Makes it easier to access and process later  \n","\n","- Tránh ghi đè dữ liệu trong các lần chạy khác nhau  \n","  Prevents data overwriting between runs  \n","\n","---\n","\n","###  Cấu trúc ví dụ:\n","###  Example structure:\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c2f26ed3","metadata":{"id":"c2f26ed3"},"outputs":[],"source":["# Creat folder (Tạo thư mục)\n","CRAWL_FOLDER = 'data/crawl_data'\n","OUTPUT_FOLDER = 'data/news_vnexpress'\n","\n","os.makedirs('data', exist_ok=True)\n","os.makedirs(CRAWL_FOLDER, exist_ok=True)\n","os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n","\n","print(f\" Created folder - (Đã tạo thư mục): {CRAWL_FOLDER}\")\n","print(f\" Create folder - (Đã tạo thư mục:) {OUTPUT_FOLDER}\")"]},{"cell_type":"markdown","id":"f3abc807","metadata":{"id":"f3abc807"},"source":["#  Crawl nội dung bài báo chi tiết  \n","#  Detailed Article Content Crawling  \n","\n","Đây là bước **quan trọng nhất** – nơi chúng ta thu thập nội dung thực tế của từng bài báo.  \n","This is the **most important step** – where we collect the actual content of each article.  \n","\n","---\n","\n","##  Quy trình hoạt động  \n","##  How It Works  \n","\n","1. Duyệt qua từng **chủ đề**  \n","   Iterate through each **topic**  \n","2. Với mỗi **link bài báo**, gửi request và **trích xuất nội dung**  \n","   For each **article link**, send a request and **extract content**  \n","3. Lưu dữ liệu vào **file JSON** (mỗi bài báo = 1 dòng JSON)  \n","   Save data into a **JSON file** (1 article = 1 JSON line)  \n","4. Hiển thị **thống kê tiến trình và kết quả**  \n","   Display **progress statistics and results**  \n","\n","---\n","\n","##  Thời gian dự kiến  \n","##  Estimated Runtime  \n","\n","| Số lượng bài báo | Thời gian ước tính | Estimated Time |\n","|------------------|--------------------|----------------|\n","| 1 bài báo        | ~2–3 giây          | ~2–3 seconds   |\n","| 50 bài báo       | ~3–5 phút          | ~3–5 minutes   |\n","| 100 bài báo      | ~7–10 phút         | ~7–10 minutes  |\n","\n","---\n","\n","##  Thống kê theo dõi  \n","##  Tracking Metrics  \n","\n","| Loại kết quả | Mô tả | Description |\n","|---------------|-------|--------------|\n","|  **Thành công** | Bài báo crawl được đầy đủ nội dung | Successfully crawled full article |\n","|  **Thất bại** | Bài báo không truy cập được hoặc không có nội dung | Article unavailable or missing content |\n","|  **Tỷ lệ thành công** | Nên đạt ít nhất 60–70% | Should reach at least 60–70% success rate |\n","\n","---\n","\n","##  Cơ chế bảo vệ  \n","##  Protection Mechanisms  \n","\n","-  **Delay 1.5 giây** giữa mỗi request để tránh bị chặn  \n","  Add a **1.5-second delay** between requests to avoid being blocked  \n","\n","-  **Error handling**: Tự động bỏ qua các bài báo lỗi  \n","  **Error handling**: Automatically skip failed articles  \n","\n","-  **Progress tracking**: Hiển thị tiến trình realtime với thanh `tqdm`  \n","  **Progress tracking**: Display realtime progress bar using `tqdm`  \n","\n","---\n","\n",">  **Quan trọng**: Không tắt notebook trong khi đang crawl!  \n","> Nếu cần dừng, hãy dùng nút **“Interrupt”** của Jupyter/Colab.  \n",">\n",">  **Important**: Do not close the notebook while crawling!  \n","> If you need to stop, use the **“Interrupt”** button in Jupyter/Colab.\n"]},{"cell_type":"code","execution_count":null,"id":"2c4ca306","metadata":{"id":"2c4ca306"},"outputs":[],"source":["# Print message indicating the start of the crawling process\n","# (In thông báo bắt đầu quá trình crawl)\n","print(' Bắt đầu crawl nội dung bài báo... / Starting to crawl article content...')\n","\n","# Print a visual separator for clarity\n","# (In dòng phân cách cho dễ nhìn)\n","print(\"═\" * 60)\n","\n","# Initialize counters for overall success and failure\n","# (Khởi tạo biến đếm tổng số crawl thành công và thất bại)\n","total_success = 0  # Total successfully crawled articles (Tổng số bài đã crawl thành công)\n","total_failed = 0   # Total failed crawls (Tổng số bài crawl thất bại)\n","\n","# Loop through each topic and its corresponding article links\n","# (Lặp qua từng chủ đề và danh sách link bài báo tương ứng)\n","for topic, links in topics_links_news.items():\n","\n","    # Skip the topic if it has no links\n","    # (Bỏ qua nếu chủ đề không có link)\n","    if not links:\n","        print(f'\\n  Bỏ qua chủ đề {topic}: không có links / Skipping topic {topic}: no links found.')\n","        continue\n","\n","    # Print current topic being processed\n","    # (In tên chủ đề hiện đang được xử lý)\n","    print(f'\\n Đang crawl chủ đề: {topic.upper()} / Crawling topic: {topic.upper()}')\n","    print(f' Số lượng: {len(links)} bài báo / Number of articles: {len(links)}')\n","    print(\"─\" * 40)  # Small separator (Dòng phân cách nhỏ)\n","\n","    # Path to save raw HTML data for this topic\n","    raw_file_path = os.path.join(CRAWL_FOLDER, f'{topic}.txt')\n","\n","    # Path to save processed data (each topic has its own folder)\n","    processed_folder = os.path.join(OUTPUT_FOLDER, topic)\n","    os.makedirs(processed_folder, exist_ok=True)\n","\n","    # Counters for current topic\n","    successful_crawls = 0\n","    failed_crawls = 0\n","\n","    # Open file for writing raw HTML data\n","    with open(raw_file_path, 'w', encoding='utf-8') as f_raw:\n","\n","        # Use tqdm to display progress bar\n","        for i, link in enumerate(tqdm(links, desc=f\"Crawling {topic}\")):\n","            try:\n","                html_content = get_content_enhanced(link)\n","\n","                if html_content is None or len(html_content) < 1000:\n","                    failed_crawls += 1\n","                    if i < 2:\n","                        print(f\" #{i+1}: Không lấy được HTML - {link[:60]}... / Failed to get HTML - {link[:60]}...\")\n","                    continue\n","\n","                raw_data = {\n","                    \"url\": link,\n","                    \"html\": html_content,\n","                    \"timestamp\": time.time()\n","                }\n","                f_raw.write(json.dumps(raw_data, ensure_ascii=False))\n","                f_raw.write('\\n')\n","\n","                # Parse article\n","                article_data = parse_article_from_html(html_content, link)\n","\n","                if article_data is not None:\n","                    output_file = os.path.join(processed_folder, str(successful_crawls).zfill(5) + \".txt\")\n","                    with open(output_file, 'w', encoding='utf-8') as f_processed:\n","                        f_processed.write(article_data['contents'])\n","                        f_processed.write('\\n')\n","\n","                    successful_crawls += 1\n","                    if i < 2:\n","                        print(f\" #{i+1}: {article_data['title'][:60]}... / Title: {article_data['title'][:60]}...\")\n","                else:\n","                    failed_crawls += 1\n","                    if i < 2:\n","                        print(f\" #{i+1}: Không trích xuất được - {link[:60]}... / Could not extract content - {link[:60]}...\")\n","\n","                time.sleep(1.5)\n","\n","            except Exception as e:\n","                failed_crawls += 1\n","                if i < 2:\n","                    print(f\" #{i+1}: Lỗi - {str(e)[:50]}... / Error - {str(e)[:50]}...\")\n","                continue\n","\n","    total_success += successful_crawls\n","    total_failed += failed_crawls\n","\n","    success_rate = successful_crawls * 100 / (successful_crawls + failed_crawls) if (successful_crawls + failed_crawls) > 0 else 0\n","\n","    print(f\"\\n Kết quả {topic}: / Results for {topic}:\")\n","    print(f\"    Thành công: {successful_crawls} / Success: {successful_crawls}\")\n","    print(f\"    Thất bại: {failed_crawls} / Failed: {failed_crawls}\")\n","    print(f\"    Tỷ lệ: {success_rate:.1f}% / Success rate: {success_rate:.1f}%\")\n","\n","# After all topics processed\n","print(\"\\n\" + \"═\" * 60)\n","print(f\" TỔNG KẾT CRAWL / FINAL CRAWL SUMMARY\")\n","print(\"═\" * 60)\n","print(f\" Tổng thành công: {total_success} / Total success: {total_success}\")\n","print(f\" Tổng thất bại: {total_failed} / Total failed: {total_failed}\")\n","\n","total_rate = total_success * 100 / (total_success + total_failed) if (total_success + total_failed) > 0 else 0\n","print(f\" Tỷ lệ thành công: {total_rate:.1f}% / Overall success rate: {total_rate:.1f}%\")\n","\n","if total_rate > 50:\n","    print(\"\\n Crawl thành công! Tiếp tục với bước tiếp theo. / Crawl successful! Proceed to the next step.\")\n","else:\n","    print(\"\\n Tỷ lệ thành công thấp, cần kiểm tra lại. / Low success rate, please review the process.\")\n"]},{"cell_type":"markdown","id":"0b8fd30b","metadata":{"id":"0b8fd30b"},"source":["#  Lưu dữ liệu  \n","#  Save Data  \n","\n","---\n","\n","##  Kiểm tra kết quả crawl  \n","##  Check Crawling Results  \n","\n","Sau khi hoàn tất quá trình crawl, chúng ta đã có:  \n","After completing the crawling process, we now have:  \n","\n","- **`data/crawl_data/`** → Chứa **dữ liệu thô (HTML)** ở dạng JSON – mỗi dòng là một bài báo gồm URL và toàn bộ HTML  \n","  → Contains **raw HTML data** in JSON format – each line represents an article with its URL and full HTML  \n","\n","- **`data/news_vnexpress/`** → Chứa **dữ liệu đã xử lý (text sạch)** – mỗi file `.txt` là nội dung một bài báo  \n","  → Contains **clean processed data** – each `.txt` file holds the cleaned article content  \n","\n","---\n","\n","##  Cấu trúc thư mục  \n","##  Folder Structure  \n","\n","\n","```\n","data/\n","├── crawl_data/          # Dữ liệu thô (HTML)\n","│   ├── khoa-hoc.txt    # JSON: {url, html, timestamp}\n","│   ├── the-thao.txt\n","│   └── doi-song.txt\n","└── news_vnexpress/      # Dữ liệu đã xử lý (text)\n","    ├── khoa-hoc/\n","    │   ├── 00000.txt    # Nội dung bài báo đã trích xuất\n","    │   ├── 00001.txt\n","    │   └── ...\n","    ├── the-thao/\n","    └── doi-song/\n","```\n","\n","\n","---\n","\n","##  Lợi ích của cấu trúc này  \n","##  Benefits of This Structure  \n","\n","| Tiếng Việt | English |\n","|-------------|----------|\n","| **Dữ liệu thô được bảo toàn** – có thể xử lý lại nếu cần thay đổi cách trích xuất | **Raw data preserved** – can be reprocessed if extraction logic changes |\n","| **Dữ liệu sạch sẵn sàng cho ML/NLP** | **Clean data ready for ML/NLP tasks** |\n","| **Phân loại rõ ràng** – mỗi thư mục = 1 class | **Clear categorization** – each folder = one class |\n","| **Linh hoạt** – dễ dàng thêm, xóa hoặc cập nhật | **Flexible** – easy to add, remove, or reprocess |\n","\n","---\n","\n",">  **Ứng dụng tiếp theo / Next Step:**  \n","> Dữ liệu trong thư mục `news_vnexpress` có thể dùng ngay để:  \n","> The data inside `news_vnexpress` can now be used for:  \n","> -  Huấn luyện mô hình phân loại văn bản / Train text classification models  \n","> -  Phân tích cảm xúc / Perform sentiment analysis  \n","> -  Tạo chatbot trả lời theo nội dung báo / Build a chatbot using news content  \n"]},{"cell_type":"code","execution_count":null,"id":"1c2ab8a4","metadata":{"id":"1c2ab8a4"},"outputs":[],"source":["# Print the heading for crawl data summary\n","# (In tiêu đề kiểm tra kết quả crawl)\n","print(' Thống kê dữ liệu đã crawl / Crawl data summary:')\n","print(\"═\" * 60)  # Print a separator line for better readability (In dòng kẻ dài để phân tách dễ nhìn)\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 1: Check raw HTML data — stored in CRAWL_FOLDER\n","# (PHẦN 1: Kiểm tra dữ liệu thô (HTML) — nằm trong thư mục CRAWL_FOLDER)\n","# ───────────────────────────────────────────────────────────────\n","print(\"\\n [1] Dữ liệu thô (HTML) trong crawl_data / Raw HTML data in crawl_data:\")\n","\n","# Iterate through all files inside the raw data folder (usually \"crawl_data\")\n","# (Duyệt qua tất cả các file trong thư mục chứa dữ liệu thô, thường là \"crawl_data\")\n","for filename in os.listdir(CRAWL_FOLDER):\n","\n","    # Only process files that end with .txt (each file represents one topic)\n","    # (Chỉ lấy các file có phần mở rộng .txt — mỗi file tương ứng một chủ đề)\n","    if filename.endswith('.txt'):\n","\n","        # Create the full file path (e.g., crawl_data/the-thao.txt)\n","        # (Tạo đường dẫn đầy đủ tới file, ví dụ: crawl_data/the-thao.txt)\n","        filepath = os.path.join(CRAWL_FOLDER, filename)\n","\n","        # Open the file and read all lines (each line = one article in JSON format)\n","        # (Mở file và đọc toàn bộ các dòng — mỗi dòng là một bài báo dạng JSON)\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            lines = f.readlines()\n","\n","        # Print the number of crawled articles (based on number of lines)\n","        # (In ra số lượng bài báo theo số dòng trong file)\n","        print(f\"  - {filename}: {len(lines)} bài báo (HTML) / {len(lines)} articles (HTML)\")\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 2: Check processed text data — stored in OUTPUT_FOLDER\n","# (PHẦN 2: Kiểm tra dữ liệu đã xử lý (text) — nằm trong thư mục OUTPUT_FOLDER)\n","# ───────────────────────────────────────────────────────────────\n","print(\"\\n [2] Dữ liệu đã xử lý (text) trong news_vnexpress / Processed text data in news_vnexpress:\")\n","\n","# Iterate through each subfolder inside OUTPUT_FOLDER\n","# (Duyệt qua từng thư mục con trong OUTPUT_FOLDER)\n","# Each subfolder represents a topic (e.g., “the-thao”, “giao-duc”)\n","# (Mỗi thư mục con tương ứng với một chủ đề như “the-thao”, “giao-duc”)\n","for topic_folder in os.listdir(OUTPUT_FOLDER):\n","\n","    # Create full path to this topic’s folder\n","    # (Tạo đường dẫn đầy đủ đến thư mục chủ đề)\n","    topic_path = os.path.join(OUTPUT_FOLDER, topic_folder)\n","\n","    # Ensure it's a folder (skip any files accidentally placed here)\n","    # (Kiểm tra nếu đúng là thư mục, tránh lỗi nếu có file lẻ)\n","    if os.path.isdir(topic_path):\n","\n","        # Get list of all .txt files inside this folder (each file = one article)\n","        # (Lấy danh sách các file .txt trong thư mục này — mỗi file là một bài báo)\n","        files = [f for f in os.listdir(topic_path) if f.endswith('.txt')]\n","\n","        # Print topic name and number of processed text files\n","        # (In tên chủ đề và số lượng file text bên trong)\n","        print(f\"  - {topic_folder}/: {len(files)} file text / {len(files)} text files\")\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 3: Print a short summary explaining the data structure\n","# (PHẦN 3: In thông báo tóm tắt cấu trúc dữ liệu)\n","# ───────────────────────────────────────────────────────────────\n","print(\"\\n Hoàn thành! Dữ liệu đã được tổ chức theo cấu trúc: / Done! Data is organized as follows:\")\n","\n","# Provide a short explanation for each data folder\n","# (Giải thích ngắn gọn để người đọc hiểu từng loại dữ liệu)\n","print(\"  - crawl_data: Dữ liệu thô (HTML + metadata) / Raw data (HTML + metadata)\")\n","print(\"  - news_vnexpress: Dữ liệu đã xử lý (text sạch) / Processed data (clean text only)\")\n"]},{"cell_type":"markdown","id":"dadd3a20","metadata":{"id":"dadd3a20"},"source":["## Thống kê và xuất kết quả\n","## Statistics & Export Results\n","\n","Cell cuối cùng này sẽ thực hiện / This final cell will:\n","\n","1. **Thống kê tổng quan** – Số lượng bài báo theo từng chủ đề  \n","   **Overview statistics** – Number of articles per topic\n","\n","2. **Hiển thị mẫu dữ liệu** – Preview nội dung đã crawl được  \n","   **Show sample data** – Preview the crawled content\n","\n","3. **Tạo file ZIP** – (Chỉ trên Google Colab) Đóng gói data để download  \n","   **Create ZIP file** – (Only in Google Colab) Package data for download\n","\n","4. **Đánh giá kết quả** – Crawl có thành công hay không  \n","   **Evaluate results** – Check if crawling was successful\n","\n","---\n","\n","### Tiêu chí đánh giá thành công\n","### Success Criteria\n","\n","- **Số lượng**: Có ít nhất 20-30 bài báo mỗi chủ đề  \n","  **Quantity**: At least 20-30 articles per topic\n","\n","- **Chất lượng**: Nội dung có ý nghĩa, không phải HTML rác  \n","  **Quality**: Meaningful content, not raw HTML\n","\n","- **Đa dạng**: Các chủ đề khác nhau có nội dung phù hợp  \n","  **Diversity**: Different topics contain relevant content\n","\n","---\n","\n","### File xuất ra\n","### Output Files\n","\n","- **vnexpress_crawl_data.zip**: Chứa toàn bộ dữ liệu crawl được  \n","  **vnexpress_crawl_data.zip**: Contains all crawled data\n","\n","- **Cấu trúc đầy đủ**: Bao gồm cả raw data và formatted data  \n","  **Complete structure**: Includes both raw and formatted data\n","\n","---\n","\n",">  **Chúc mừng**: Nếu crawl thành công, bạn đã có một dataset tin tức tiếng Việt hoàn chỉnh để sử dụng cho các dự án ML/NLP!  \n",">  **Congratulations**: If crawling is successful, you now have a complete Vietnamese news dataset ready for ML/NLP projects!\n"]},{"cell_type":"code","execution_count":null,"id":"a2397b8f","metadata":{"id":"a2397b8f"},"outputs":[],"source":["# Print final summary header\n","# (In tiêu đề phần thống kê cuối cùng)\n","print(\" FINAL CRAWL SUMMARY / THỐNG KÊ KẾT QUẢ CUỐI CÙNG\")\n","print(\"═\" * 50)  # Print a separator line for clarity (In một đường kẻ dài cho rõ ràng)\n","\n","# Initialize counters for total raw and formatted articles\n","# (Khởi tạo biến đếm tổng số bài báo thô và bài đã xử lý)\n","raw_total = 0\n","formatted_total = 0\n","\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 1: Count raw data (JSON or HTML files stored in crawl_data)\n","# (PHẦN 1: Đếm dữ liệu thô (JSON hoặc HTML thô lưu trong crawl_data))\n","# ───────────────────────────────────────────────────────────────\n","print(\"\\n RAW DATA (JSON) / DỮ LIỆU THÔ (JSON):\")\n","\n","# Loop through all files in the folder containing raw crawled data\n","# (Duyệt qua tất cả file trong thư mục chứa dữ liệu thô)\n","for filename in sorted(os.listdir(CRAWL_FOLDER)):\n","    if filename.endswith('.txt'):\n","        filepath = os.path.join(CRAWL_FOLDER, filename)\n","        try:\n","            with open(filepath, 'r', encoding='utf-8') as f:\n","                lines = [line.strip() for line in f.readlines() if line.strip()]\n","\n","            # Print the number of articles (counted by lines)\n","            # (In ra số lượng bài báo (tính theo số dòng))\n","            print(f\"    {filename}: {len(lines)} articles / {len(lines)} bài báo\")\n","\n","            raw_total += len(lines)\n","        except:\n","            print(f\"    {filename}: File read error / Lỗi đọc file\")\n","\n","# Print total number of raw articles crawled\n","# (In tổng số bài báo thô đã crawl được)\n","print(f\"\\n TOTAL RAW DATA: {raw_total} articles / Tổng dữ liệu thô: {raw_total} bài báo\")\n","\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 2: Count processed data (clean text in news_vnexpress)\n","# (PHẦN 2: Đếm dữ liệu đã xử lý (text sạch trong news_vnexpress))\n","# ───────────────────────────────────────────────────────────────\n","print(\"\\n FORMATTED DATA / DỮ LIỆU ĐÃ ĐỊNH DẠNG:\")\n","\n","for folder_name in sorted(os.listdir(OUTPUT_FOLDER)):\n","    folder_path = os.path.join(OUTPUT_FOLDER, folder_name)\n","    if os.path.isdir(folder_path):\n","        files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n","        print(f\"    {folder_name}: {len(files)} files / {len(files)} file\")\n","        formatted_total += len(files)\n","\n","print(f\"\\n TOTAL FORMATTED FILES: {formatted_total} / Tổng file đã định dạng: {formatted_total}\")\n","\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 3: Final result summary\n","# (PHẦN 3: Đưa ra kết luận tổng thể)\n","# ───────────────────────────────────────────────────────────────\n","if formatted_total > 0:\n","    print(\"\\n\" + \"═\" * 50)\n","    print(\" CRAWL SUCCESSFUL! / CRAWL THÀNH CÔNG!\")\n","    print(\"═\" * 50)\n","    print(f\" {formatted_total} articles collected / Đã crawl được {formatted_total} bài báo\")\n","    print(\" Data saved at: data/news_vnexpress/ / Dữ liệu lưu tại: data/news_vnexpress/\")\n","\n","    # ───────────────────────────────────────────────────────────────\n","    # PART 4: If running in Google Colab → compress data for download\n","    # (PHẦN 4: Nếu đang chạy trong Google Colab thì nén dữ liệu thành file zip)\n","    # ───────────────────────────────────────────────────────────────\n","    try:\n","        import google.colab\n","        IN_COLAB = True\n","    except:\n","        IN_COLAB = False\n","\n","    if IN_COLAB:\n","        print(\"\\n Creating ZIP file for Google Colab... / Đang tạo file zip cho Google Colab...\")\n","        import zipfile\n","\n","        zip_filename = 'vnexpress_crawl_data.zip'\n","        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            for root, dirs, files in os.walk('data'):\n","                for file in files:\n","                    file_path = os.path.join(root, file)\n","                    arcname = os.path.relpath(file_path, '.')\n","                    zipf.write(file_path, arcname)\n","\n","        print(f\" {zip_filename} created / Đã tạo {zip_filename}\")\n","\n","        try:\n","            from google.colab import files\n","            files.download(zip_filename)\n","            print(\" Downloading file... / File đang được tải xuống...\")\n","        except:\n","            print(\" Please download manually from Files panel / Vui lòng tải thủ công từ panel Files\")\n","\n","    # ───────────────────────────────────────────────────────────────\n","    # PART 5: Show one text sample for quick inspection\n","    # (PHẦN 5: Hiển thị một mẫu dữ liệu text để kiểm tra)\n","    # ───────────────────────────────────────────────────────────────\n","    print(\"\\n SAMPLE DATA / MẪU DỮ LIỆU:\")\n","    sample_found = False\n","\n","    for folder_name in os.listdir(OUTPUT_FOLDER):\n","        folder_path = os.path.join(OUTPUT_FOLDER, folder_name)\n","        if os.path.isdir(folder_path):\n","            files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n","            if files:\n","                sample_file = os.path.join(folder_path, files[0])\n","                try:\n","                    with open(sample_file, 'r', encoding='utf-8') as f:\n","                        content = f.read()[:300]\n","                    print(f\"\\n Topic '{folder_name}' - File '{files[0]}': / Chủ đề '{folder_name}' - File '{files[0]}':\")\n","                    print(f\"{content}...\")\n","                    sample_found = True\n","                    break\n","                except:\n","                    continue\n","\n","    if not sample_found:\n","        print(\" No sample data found / Không tìm thấy mẫu dữ liệu\")\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 6: If no data → show failure message\n","# (PHẦN 6: Nếu không có dữ liệu nào => thông báo thất bại)\n","# ───────────────────────────────────────────────────────────────\n","else:\n","    print(\"\\n\" + \"═\" * 50)\n","    print(\" CRAWL FAILED / CRAWL THẤT BẠI\")\n","    print(\"═\" * 50)\n","    print(\" No data was saved successfully / Không có dữ liệu nào được lưu thành công\")\n","    print(\" Suggestions / Gợi ý:\")\n","    print(\"   - Check internet connection / Kiểm tra kết nối internet\")\n","    print(\"   - Try again in a few minutes / Thử lại sau vài phút\")\n","    print(\"   - Recheck Test Step (Step 5) / Kiểm tra lại bước Test (Bước 5)\")\n","\n","# ───────────────────────────────────────────────────────────────\n","# PART 7: End of program\n","# (PHẦN 7: Kết thúc chương trình)\n","# ───────────────────────────────────────────────────────────────\n","print(\"\\n ALL STEPS COMPLETED! / HOÀN THÀNH TẤT CẢ CÁC BƯỚC!\")\n"]},{"cell_type":"markdown","id":"62df045b","metadata":{"id":"62df045b"},"source":["## Xử lý lỗi và Troubleshooting\n","##  Error Handling & Troubleshooting\n","\n","### Các lỗi thường gặp và cách khắc phục\n","### Common Errors and Solutions\n","\n","#### 1. **ConnectionError / TimeoutError**\n","#### 1. **ConnectionError / TimeoutError**\n","\n","```\n","requests.exceptions.ConnectionError: HTTPSConnection pool\n","```\n","**Nguyên nhân**: Mạng không ổn định hoặc website đang down  \n","**Cause**: Unstable network or website downtime  \n","\n","**Khắc phục / Solution**:  \n","- Kiểm tra kết nối internet / Check your internet connection  \n","- Thử lại sau 5-10 phút / Retry after 5-10 minutes  \n","- Giảm số lượng request đồng thời / Reduce concurrent requests  \n","\n","#### 2. **HTTP 403 Forbidden**\n","#### 2. **HTTP 403 Forbidden**\n","\n","```\n","HTTP 403 cho https://vnexpress.net/...\n","```\n","**Nguyên nhân**: Website chặn bot/crawler  \n","**Cause**: Website blocks bots/crawlers  \n","\n","**Khắc phục / Solution**:  \n","- Tăng delay giữa các request: `time.sleep(3)` / Increase delay between requests  \n","- Thay đổi User-Agent / Change User-Agent  \n","- Sử dụng phương pháp cURL backup / Use cURL as backup  \n","\n","#### 3. **HTTP 429 Too Many Requests**\n","#### 3. **HTTP 429 Too Many Requests**\n","\n","```\n","HTTP 429 cho https://vnexpress.net/...\n","```\n","**Nguyên nhân**: Gửi request quá nhanh, bị rate limit  \n","**Cause**: Requests sent too fast, rate-limited  \n","\n","**Khắc phục / Solution**:  \n","- Tăng delay lên 5-10 giây / Increase delay to 5-10 seconds  \n","- Crawl từng chủ đề một, không crawl song song / Crawl topics sequentially, not in parallel  \n","- Chờ 30 phút rồi thử lại / Wait 30 minutes then retry  \n","\n","#### 4. **Không tìm thấy nội dung (None result)**\n","#### 4. **No content found (None result)**\n","\n","```\n","Thất bại: None\n","```\n","**Nguyên nhân**: CSS selector không đúng hoặc cấu trúc HTML thay đổi  \n","**Cause**: Incorrect CSS selector or changed HTML structure  \n","\n","**Khắc phục / Solution**:  \n","- Kiểm tra HTML source bằng cách mở URL trên browser / Check HTML source in browser  \n","- Cập nhật CSS selectors trong code / Update CSS selectors in code  \n","- Test với ít bài báo trước / Test with a few articles first  \n","\n","#### 5. **Memory Error / Out of Memory**\n","#### 5. **Memory Error / Out of Memory**\n","\n","```\n","MemoryError: Unable to allocate array\n","```\n","**Nguyên nhân**: Crawl quá nhiều dữ liệu cùng lúc  \n","**Cause**: Crawling too much data at once  \n","\n","**Khắc phục / Solution**:  \n","- Giảm số lượng chủ đề / Reduce number of topics  \n","- Crawl từng batch nhỏ / Crawl in smaller batches  \n","- Clear memory định kỳ: `import gc; gc.collect()` / Periodically clear memory  \n","\n","---\n","\n","### Debug Commands hữu ích\n","### Useful Debug Commands\n","\n","```python\n","# Simple connection test / Test kết nối đơn giản\n","import requests\n","\n","# Send a GET request to VnExpress homepage / Gửi yêu cầu GET đến trang chủ VnExpress\n","response = requests.get('https://vnexpress.net/')\n","\n","# Print the HTTP response status code / In mã trạng thái HTTP trả về\n","print(f\"Status: {response.status_code}\")\n","\n","# ───────────────────────────────────────────────────────────────\n","# Check if IP is blocked using curl command / Kiểm tra IP có bị chặn không bằng lệnh curl\n","# ───────────────────────────────────────────────────────────────\n","import subprocess\n","\n","# Run the command: curl -I https://vnexpress.net/ / Chạy lệnh curl -I để chỉ lấy phần header\n","result = subprocess.run(['curl', '-I', 'https://vnexpress.net/'],\n","                       capture_output=True,  # Capture output / Lưu kết quả\n","                       text=True)  # Decode as text / Giải mã thành chữ\n","\n","# Print the HTTP headers returned by the server / In header trả về\n","print(result.stdout)\n","```\n","\n","###  Checklist khi gặp lỗi:\n","\n","### Checklist When Facing Errors\n","\n","1. Kiểm tra mạng: Có truy cập được VNExpress bằng browser không?\n","Check network: Can you access VNExpress via browser?\n","\n","2. Kiểm tra code: Có lỗi syntax hoặc typo không?\n","Check code: Any syntax errors or typos?\n","\n","3. Kiểm tra delay: Có để delay đủ lớn không? (ít nhất 1-2 giây)\n","Check delay: Is the delay sufficient (at least 1-2 sec)?\n","\n","4. Kiểm tra số lượng: Có crawl quá nhiều cùng lúc không?\n","Check quantity: Are you crawling too many articles at once?\n","\n","5. Kiểm tra thời gian: Có crawl vào giờ cao điểm không?\n","Check timing: Are you crawling during peak hours?\n","\n","###  Tips tối ưu hóa:\n","\n","- **Crawl vào ban đêm** (0h-6h) thường ít bị chặn hơn / Crawling at night (0h-6h) usually less blocked\n","- **Bắt đầu với 1 chủ đề** để test trước / Start with 1 topic for testing\n","- **Lưu progress** thường xuyên để tránh mất dữ liệu / Save progress frequently\n","- **Monitor resource usage** (CPU, RAM, Network) / Monitor CPU, RAM, and network usage\n","\n","> **Khi nào cần dừng**: Nếu tỷ lệ thành công < 30% hoặc liên tục gặp HTTP 403/429, hãy dừng và thử lại sau ít nhất 1 giờ.\n","> **When to stop**: If success rate < 30% or continuous HTTP 403/429, stop and retry after at least 1 hour."]},{"cell_type":"markdown","id":"487387fd","metadata":{"id":"487387fd"},"source":["# Kết luận\n","# Conclusion\n","\n","## Chúc mừng bạn đã hoàn thành!\n","## Congratulations on Completing!\n","\n","Bạn vừa trải qua một hành trình hoàn chỉnh về **Web Crawling** - từ việc hiểu concepts cơ bản đến implement một crawler thực tế có thể thu thập hàng trăm bài báo!  \n","You have just completed a full journey in **Web Crawling** – from understanding the basic concepts to implementing a real crawler that can collect hundreds of articles!\n","\n","## Những gì bạn đã học được:\n","## What You Have Learned:\n","\n","### Kỹ năng kỹ thuật / Technical Skills\n","- **Web Scraping với Python / Web Scraping with Python**: Sử dụng requests, BeautifulSoup / Using requests and BeautifulSoup  \n","- **HTML parsing**: Trích xuất dữ liệu từ cấu trúc HTML phức tạp / Extract data from complex HTML structures  \n","- **Error handling**: Xử lý các lỗi thường gặp khi crawl / Handle common crawling errors  \n","- **Data processing**: Làm sạch và tổ chức dữ liệu / Clean and organize data  \n","- **Anti-detection**: Kỹ thuật tránh bị chặn bởi website / Techniques to avoid being blocked by websites  \n","\n","### Tư duy lập trình / Programming Mindset\n","- **Modular design**: Chia nhỏ vấn đề thành các hàm riêng biệt / Break down problems into modular functions  \n","- **Error resilience**: Xây dựng system có thể recover từ lỗi / Build a system resilient to errors  \n","- **Progress monitoring**: Theo dõi và báo cáo tiến trình / Monitor and report progress  \n","- **Data quality**: Đảm bảo chất lượng dữ liệu thu thập được / Ensure quality of collected data  \n","\n","### Hiểu biết về web / Web Understanding\n","- **HTTP protocols**: Cách web browser và server giao tiếp / How browsers and servers communicate  \n","- **Website structure**: Cách phân tích cấu trúc một website / How to analyze website structure  \n","- **Rate limiting**: Tại sao và cách websites bảo vệ chống crawler / Why websites limit requests and how they protect themselves  \n","- **Ethical crawling**: Crawl có trách nhiệm và tôn trọng website / Crawl responsibly and ethically  \n","\n","## Tài nguyên học tập bổ sung / Additional Learning Resources\n","\n","### Sách nên đọc / Books\n","- **\"Web Scraping with Python\"** - Ryan Mitchell  \n","- **\"Python for Data Analysis\"** - Wes McKinney  \n","- **\"Natural Language Processing with Python\"** - Steven Bird  \n","\n","### Khóa học online / Online Courses\n","- **Scrapy Course** trên Udemy / Scrapy Course on Udemy  \n","- **NLP Specialization** trên Coursera / NLP Specialization on Coursera  \n","- **Data Science Track** trên DataCamp / Data Science Track on DataCamp  \n","\n","### Tools và frameworks / Tools & Frameworks\n","- **Scrapy**: Industrial-strength crawling framework  \n","- **Selenium**: Crawl dynamic websites  \n","- **Pandas**: Data manipulation và analysis  \n","- **NLTK/SpaCy**: Natural Language Processing  \n","\n","## Câu hỏi / Questions\n","\n","### Câu hỏi 1: Web Crawling cơ bản / Question 1: Basic Web Crawling\n","**Câu hỏi / Question**: Trong bài thực hành này, chúng ta sử dụng những thư viện Python nào để thực hiện web crawling? Hãy nêu tên ít nhất 4 thư viện và giải thích vai trò của từng thư viện.  \n","Which Python libraries were used in this exercise for web crawling? Name at least 4 libraries and explain the role of each.\n","\n","### Câu hỏi 2: Xử lý lỗi trong Web Crawling / Question 2: Error Handling in Web Crawling\n","**Câu hỏi / Question**: Khi thực hiện crawl dữ liệu từ VNExpress, có những lỗi nào thường gặp và cách khắc phục như thế nào? Hãy nêu ít nhất 3 loại lỗi và cách xử lý.  \n","What common errors occur when crawling data from VNExpress and how can they be handled? Name at least 3 errors and their solutions.\n","\n","### Câu hỏi 3: Quy trình crawl và xử lý dữ liệu / Question 3: Crawling & Data Processing Workflow\n","**Câu hỏi / Question**: Mô tả quy trình hoàn chỉnh để crawl dữ liệu tin tức từ VNExpress theo hướng dẫn trong bài thực hành này, từ việc lấy links bài báo đến lưu trữ dữ liệu cuối cùng.  \n","Describe the complete workflow to crawl news data from VNExpress as instructed in this exercise, from collecting article links to storing the final data.\n","\n","## BTVN / Homework\n","\n","### Bài tập / Exercise:\n","Hoàn thiện 01 file `.ipynb` mới sử dụng chromedriver để crawl dữ liệu **title và link** của 10-20 bài báo trên đường link [https://cafef.vn/tai-chinh-quoc-te.chn](https://cafef.vn/tai-chinh-quoc-te.chn).  \n","Yêu cầu đầu ra là 01 file **excel** bao gồm **title** và **link** của các bài báo (có thể sử dụng cách crawl khác và trang web khác).  \n","\n","Complete a new `.ipynb` file using Chromedriver to crawl **title and link** of 10-20 articles from [https://cafef.vn/tai-chinh-quoc-te.chn](https://cafef.vn/tai-chinh-quoc-te.chn).  \n","The output should be an **Excel file** containing **title** and **link** of the articles (you may use other crawling methods or websites).\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}